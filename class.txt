def check_missing_col(dataframe):
    missing_col = []
    for col in dataframe.columns:
        missing_values = sum(dataframe[col].isna())
        is_missing = True if missing_values >= 1 else False
        if is_missing:
            print(f'결측치가 있는 컬럼은: {col} 입니다')
            print(f'해당 컬럼에 총 {missing_values} 개의 결측치가 존재합니다.')
            missing_col.append([col, dataframe[col].dtype])
    if missing_col == []:
        print('결측치가 존재하지 않습니다')
    return missing_col

missing_col = check_missing_col(train)

#결측치 시각화
# 파이썬에서는 .isnull()을 통해서 결측치를 확인할 수 있습니다.
total = train.isnull().sum().sort_values(ascending=False)
percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
percent_data = percent.head(20)
percent_data.plot(kind="bar", figsize = (8,6), fontsize = 10)
plt.xlabel("", fontsize = 20)
plt.ylabel("", fontsize = 20)
plt.title("Total Missing Value (%)", fontsize = 20)

#########################
missing = train.isnull().sum()
missing = missing[missing > 0]
missing.sort_values(inplace=True)

missing.plot.bar(figsize = (12,6))    

plt.xlabel("", fontsize = 20)
plt.ylabel("", fontsize = 20)
plt.title("Total Missing Value", fontsize = 20)

for i, val in enumerate(missing.values):
    plt.text(i,val,"%s"%val, horizontalalignment='center')    
plt.show()


################### 결측치 상관관계
import seaborn as sns
import matplotlib.pyplot as plt
missingdata_df  = train.columns[train.isnull().any()].tolist()
missingdata_df = ['age','fnlwgt','education.num']
train[missingdata_df]
colormap = plt.cm.PuBu
sns.heatmap(train[missingdata_df].corr(),square = True, linewidths = 0.1,
            cmap = colormap, linecolor = "white", vmax=0.8,annot=True, )
plt.title("Correlation with Missing Values", fontsize = 20)
plt.show()


###################결측치 처리 방법
결측치 처리 방법

1. Deletion  
결측치가 있는 행이나 열을 제거하는 방법입니다. 주로 결측치가 매우 적을 때 사용합니다. 그리고 완전한 데이터(MCAR)에 대해서만 분석을 진행한다고 볼 수 있습니다. 쉽고, 원래의 분포를 보존한다는 장점을 갖습니다. 한편 데이터 손실이 발생하여 의미있는 데이터를 잃을 수도 있다는 단점을 갖습니다.  

1. Heuristic Imputation  
분석가가 보편적인 사실(상식) 혹은 도메인 지식에 기반하여 임의로 결측치를 대체하는 방법입니다.

1. Mean//Median Imputation(수치형 변수)  
결측치를 평균/중앙값 등의 대표값으로 대체하는 방법입니다. 이때 대표값은 전체의 대표값일 수도, 특정 집단의 대표값일 수도 있습니다.  
쉽고, 빠르다는 장점을 갖습니다. 한편 원래의 분산을 변형시킬 수 있고, 원래의 공분산을 변형시킬 수 있다(즉 다른 변수와의 관계가 변형될 수 있다)는 단점을 갖습니다. 다른 feature 간의 상관관계가 고려되지 않는다. 단순히 결측지가 존재하는 컬럼만 고려됩니다

1. Most Frequent Value / Zero / Constant Imputation  
쉽다는 장점을 갖습니다. 원래의 분포를 변형시킬 수 있고, 결측치가 많을 경우 빈도 수가 높은 범주가 over-representation 될 수 있다는 단점을 갖습니다.
    - Most Frequent Value Imputation : 가장 빈번히 나온 값으로 대체한다. 범주형 feature에도 잘 동작합니다.  
    - Zero Imputation : 말 그대로 0으로 대체 합니다.  
    - Constant Imputation : 지정한 상수값으로 대체 합니다    
        

1. Prediction Model  
    - 모델을 통해 예측한 값으로 결측치를 대체하는 방법입니다. 
    - 예측 기법을 사용한 결측치 추정은 결측치들의 특성이 패턴을 가진다고 가정하고 진행하게 된다.
    - 결측값이 없는 컬럼들로 구성된 dataset으로 결측값이 있는 컬럼을 예측한다.
    - KNN,linear Imputation, SVM 과 같은 기계학습방법을 이용하여 대체 할 수 있습니다.
    
###################################
### 결측치 처리 방법을 선택하고 이유 설명

- 해당 Null 값은 어떻게 대체할 것인가?
 - 행 삭제? 중위수? 평균?
 - 해당 컬럼의 성격에 맞게 결측치 대체가 이루어져야 함
 - 범주형 데이터이기 때문에 중위수, 평균이 아니라 최빈값 기준으로 이루어져야 함
  - 일 유형 / 직업의 경우, 나이 / 교육수준이 비슷한 사람들끼리 연봉 수준이나 업무의 유형이 비슷할 것으로 생각되기 때문에 비슷한 데이터들의 최빈값으로 대체 고려. 
  - 국적의 경우, 해당 컬럼이 데이터 분석에서 얼마나 유의미할지는 모르겠지만 국가별 소득 수준, 교육 수준이 평균적으로 다를 것으로 생각. 해당 데이터들의 최빈값 혹은 중위수로 대체
  - CPS weights에 대한 의미 체감이 잘 되지 않지만, 해당 데이터가 중요할 것으로 생각
  - 결측치 전부가 범주형 데이터이기 때문에, 해당 데이터를 살펴보고 패턴이 없다면 제거하는 방법 고려	
  
################################## 결측치 삭제 하는 방법
# 결측치를 처리하는 함수를 작성합니다.
def handle_na(data, missing_col):
    temp = data.copy()
    for col, dtype in missing_col:
        if dtype == 'O':
            # 범주형 feature가 결측치인 경우 해당 행들을 삭제해 주었습니다.
            temp = temp.dropna(subset=[col])
    return temp

train = handle_na(train, missing_col)

# 결측치 처리가 잘 되었는지 확인해 줍니다.
missing_col = check_missing_col(train)           

################################## 종속변수 클래스 분포 확인
import numpy as np
counted_values = train['target'].value_counts()
plt.style.use('ggplot')
plt.figure(figsize=(12, 10))
plt.title('class counting', fontsize = 30)
value_bar_ax = sns.barplot(x=counted_values.index, y=counted_values)
value_bar_ax.tick_params(labelsize=20)
for i, val in enumerate(counted_values.index):    
    freq = counted_values[val]
    freq_s =  np.round((counted_values[val] / len(train)) * 100,2) 
#     print(freq_s)
    plt.text(i,freq,"%s"%freq_s+'%', horizontalalignment='center', fontsize=20)


### 종속변수 클래스 분포 해석

위에 그래프에서와 같이 이번 대회는 데이터 불균형이 심한 편입니다.

소득이 5만달러 이하인 데이터 값이 훨씬 많군요.

data를 증강시키거나 적은 양의 클래스를 따로 맞추는 모델을 만들어보는 등의 시도를 해볼 수 있을 것 같습니다.

****
<font size="5">클래스 불균형일때 정리 </font>

해당 데이터 세트의 레이블인 Class 속성은 매우 불균형한 분포를 가지고 있습니다. Class는 0과 1로 분류되는데 0이 사기가 아닌 정상적인 신용카드 트랙잭션 데이터, 1은 신용카드 사기 트랜잭션을 의미합니다. 전체 데이터의 약 0.17%만 레이블 값이 1입니다.

레이블이 블균형한 분포를 가진 데이터 세트를 학습 시킬때는 예측 성능의 문제가 발생 할 수 있는데, 이는 이상 레이블을 가지는 데이터 건수가 정상 레이블을 가진 데이터 건수에 비해 너무 적기 때문에 발생합니다. 

즉 이상 레이블을 가지는 데이터 건수는 매우 적기 때문에 제대로 다양한 유형을 학습 하지 못하는 반면에 정상 레이블을 가지는 데이터 건수는 매우 많기 때문에 일방적으로 정상 레이블로 치우친 학습을 수행해 제대로 된 이상 데이터 검출이 어려워 지기 쉽습니다. (0으로 예측하는 것에 과대적합할 가능성이 많습니다.)

지도 학습에서 극도로 불균형한 레이블 값 분포로 인한 문제점을 해결하기 위해서는 적절한 학습 데이터를 확보하는 방안이 필요한데 , 대표적으로 오버 샘플링 및 언더 샘플링 방법이 있으며, 오버 샘플링 방식이 예측 성능상 더 유리한 경우가 많아 주로 사용됩니다.
######################################################################################################

#################################범주형 변수별 그룹 확인
from IPython.display import display
cate_feat = []
num_feat = []
for col in train.columns:
    target = train[col]
    if target.nunique() <=20:
        print(col,target.unique())
        display(target.value_counts().to_frame())
        print()
        cate_feat.append(col)
    else:
        num_feat.append(col)
print('범주형 :', cate_feat)
print('연속형: ', num_feat)
#################################
import matplotlib.pyplot as plt
from matplotlib import rcParams

# setting chart design
colors = ['#ff9999', '#ffc000', '#8fd9b6', '#d395d0']
wedgeprops={'width': 0.7, 'edgecolor': 'w', 'linewidth': 5}

# pie chart는 matplotlib에만 있음
for i in range(len(cate_feat)):    
    plt.pie(train[cate_feat[i]].value_counts(), \
            labels=train[cate_feat[i]].value_counts().keys(), autopct='%.1f%%', \
            startangle=260, counterclock=False, colors=colors, wedgeprops=wedgeprops)
    plt.title(cate_feat[i])
    plt.show()

#################################범주형 피쳐 데이터 시각화
train_categori = train.drop(['id', 'age', 'fnlwgt', 'education.num', 
                             'capital.gain', 'capital.loss', 'hours.per.week'],axis = 1) #범주형이 아닌 피쳐 drop
train_categori.head()

category_feature = [ col for col in train.columns if train[col].dtypes == "object"]
category_feature.append('target')
train_categori = train[category_feature]
train_categori.head()

# 범주형 데이터 분포 
def visualize(axx, field, num): ##그래프를 그리기 위한 메소드
    sns.countplot(train_categori.columns[num], 
                  data= train_categori[train_categori['target'] == field], 
                  color='#eaa18a', ax = axx) # countplot을 이용하여 그래프를 그려줍니다.
    axx.set_title(field)

## 원하는 개수의 subplots 만들어주기    
figure, ((ax1,ax2),(ax3,ax4), (ax5, ax6),(ax7, ax8), (ax9, ax10),
         (ax11,ax12),(ax13,ax14), (ax15, ax16))  = plt.subplots(nrows=8, ncols=2) 
figure.set_size_inches(40, 50) #(w,h)
figure.suptitle('Compare categorical features', fontsize=40, y = 0.9)

k = 0 # 피쳐 수
j = 1 # 그래프 수
while k<8: 
    for i in range(0,2):
        visualize(eval(f'ax{j}'), train_categori['target'].unique()[i], k)
        j = j+1
    k = k+1
	
#########################################################
import seaborn as sns
import matplotlib.pyplot as plt

for i in cate_feat:

    f, ax = plt.subplots(1,2,figsize=(12,6))
    
    sns.countplot(x=i,hue='target',data=train, ax=ax[0])
    plt.sca(ax[0])
    plt.xticks(rotation=90)

    sns.pointplot(x=i,y='target',data=train, ax=ax[1],join=False)
    plt.sca(ax[1])
    plt.xticks(rotation=90)

    plt.show()

<font size="5">범주형 변수 시각화 설명</font>

*** 5만 이하는 저소득 / 5만 이상은 고소득으로 간단히 명명 ***

1. WorkClass
 - 사기업 종사자가 대부분, 자영업, 지방, 주정부 순으로 많은 것을 확인

2. Education
 - 저소득의 경우, 고졸 / 전문대 / 학사 순으로 많다
 - 고소득의 경우, 학사 / 고졸 / 전문대 순으로 많음
   - 교육수준이 소득과 어느정도의 연관성이 있음을 반증

3. Marital Status
 - 저소득의 경우, 미혼 / 기혼 / 이혼 순
 - 고소득의 경우, 기혼 / 이혼 / 미혼 순
   - 소득이 결혼을 하는데 있어 중요한 선택지임을 나타내는 지표라는 생각이 든다

4. Occupation
 - 저소득의 경우, Adm-clerical / Craft-repair / Other Service 순
 - 고소득의 경우, Exec-managerial / Prof-specialty / Sales 순
   - 연봉이 높은 직업과 낮은 직업 별로 분포가 되어있다	
   
#########################################################범주형 변수 상관관계
from scipy.stats import chi2_contingency, chisquare
import seaborn as sns 

corr_list= []

for i in cate_feat:
    c_list = []
    for j in cate_feat:
        ct = pd.crosstab(train[i],train[j])
        X2=chi2_contingency(observed=ct)[0]
        n = len(train)
        minDim = min(len(train[i].unique()),len(train[j].unique()))-1
        c = np.sqrt((X2/n) / minDim)
#         c = np.sqrt(result[0]/(len(train)*(min(len(train[i].unique()),
#                len(train[j].unique()))-1)))
        c_list.append(c)
    corr_list.append(c_list)
corr_df = pd.DataFrame(corr_list,columns=cate_feat, index=cate_feat)

sns.set(rc = {'figure.figsize':(20,12)})
sns.heatmap(corr_df,vmin=-1,vmax=1,cmap='RdBu',linewidths=.1,annot=True, fmt='.2f')

#### 범주형 변수 상관관계 해석

크래머 V계수 효과 크기의 해석

|효과 크기(ES)|해석|
|------|---|
|ES ≤ 0.2|결과가 약합니다. 결과가 통계적으로 유의함에도 불구하고 필드들은 약하게만 연관됩니다.|
|0.2 < ES ≤ 0.6|결과가 적당합니다. 필드들은 적당하게 연관됩니다.|
|ES > 0.6|결과가 강력합니다. 필드들이 강력하게 연관됩니다.|


***

상관분석 종류

- 연속형 변수 
    1. 피어슨 상관분석    
    2. 스피어만 상관분석     
    3. 켄달 상관분석
- 범주형 변수
    1. 파이(Phi) 상관계수
    2. 크래머 V(Cramer's V) 계수    

|구분|파이 상관계수|크래머 V계수|
|------|---|---|
|자료척도|명목 척도|명목 척도|
|자료 형태|비교대상 변수 모두 범주가 2개(남/여),(O/X)|비교 대상 변수 범주가 3개 이상(10대/20대/30대),(단독/연립/아파트)|
|결과값|Phi 상관계수|Cramer's V 계수|
|결과 범위|0 ~ +1|0 ~ +1|

#########################################################수치형 피쳐 데이터 시각화
#수치형 피쳐와 label인 target 추출
train_numeric = train[['age', 'fnlwgt', 'capital.gain', 'capital.loss', 'hours.per.week', 'target']] 
train_numeric.head()
   
numerical_feature = [ col for col in train.columns if train[col].dtypes != "object"]
numerical_feature.remove('id') 
numerical_feature.remove('education.num') 
# numerical_feature.append('target')
train_numeric = train[numerical_feature]
train_numeric.head()   
   
# 수치형 데이터 분포
def visualize(axx, field, num):
    line = train_numeric[train_numeric['target'] == field] #메소드에서 target 클래스 추춣
    name = train_numeric[train_numeric['target'] == field][train_numeric.columns[num]].name #메소드에서 이름 추출
#     sns.kdeplot(x = line[train_numeric.columns[num]],  data = train_numeric, ax = axx, color='#eaa18a') 
    #countplot을 이용하여 그래프를 그려줍니다.
    sns.distplot(line[train_numeric.columns[num]],  ax = axx, color='#eaa18a') 
    axx.axvline(line.describe()[name]['mean'], c='#f55354', 
                label = f"mean = {round(line.describe()[name]['mean'], 2)}") #mean 통계값을 표기해줍니다.
    axx.axvline(line.describe()[name]['50%'], c='#518d7d', 
                label = f"median = {round(line.describe()[name]['50%'], 2)}") #median 통계값을 표기해줍니다.
    axx.legend()
    axx.set_title(field)

##원하는 개수의 subplots 만들어주기    
figure, ((ax1,ax2),(ax3,ax4), (ax5, ax6),(ax7, ax8), (ax9, ax10))  = plt.subplots(nrows=5, ncols=2) 
figure.set_size_inches(40, 50) #(w,h)
figure.suptitle('Compare numeric features', fontsize=40, y = 0.9)

k = 0 # 피쳐 수
j = 1 # 그래프 수
while k<5:
    for i in range(0,2):
        visualize(eval(f'ax{j}'), train_numeric['target'].unique()[i], k)
        j = j+1
    k = k+1
	
##################################################################
import seaborn as sns

for i in numerical_feature:

    f, ax = plt.subplots(1,2,figsize=(12,6))
    
    sns.histplot(x=train[i],bins=10,hue=train['target'], ax=ax[0])    
    
    sns.histplot(x=np.log1p(train[i]),bins=10,hue=train['target'], ax=ax[1])

    plt.show()
	
###################그래프 해석
age 그래프를 보면 

<=50k 에 비하여  >50k에서 나이가 더 많은 분포 형상을 뛰고 있습니다. 

또한 mean값을 살펴보면 capital.gain, capital.loss, hours.per.week가 >50k에서 더 높은 수치를 가지고 있는 것을 확인 할 수 있습니다.

따라서 age, capital.gain, capital.loss, hours.per.week 피쳐에서 수치가 클수록 수입의 영향이 많이 미칠 것이라고 예상이 됩니다.

1. Age
 - 저소득
   - 20대에서 40대사이 주로 분포하는 것으로 확인
 - 고소득
   - 주로 30대 후반 ~ 50대 사이 분포하는 것으로 확인
 - 평균적으로 소득이 가장 많을 연령대가 4~50대이기 때문

2. fnlwgt
 - 가중치의 양상이 저소득과 고소득 간 분포가 비슷한데, 해당 가중치의 기준이 정확히 무엇인지 모르겠다

3. hours.per.week
 - 고소득이 비교적 높은 주당 업무시간을 소요. 
 - 저소득의 경우 주 40시간을 기준으로 주로 분포, 고소득의 경우에는 40시간 이상에도 어느정도 밀집군을 형성하고 있는 것으로 확인

- 정규분포의 형태를 띄는 변수들도 있지만, 치우쳐져 있는 변수들이 많다. 정규화를 할 때 고려해야 할 듯.


train.agg(['skew','kurtosis']).T
# skewness: -2 ~ +2 사이면 왜도가 크지 않다고 판단함.
# urtosis: 첨도가 높을수록 이상치가 많아짐.

#### 히스토그램 및 분포
- id의 경우 모든 행이 다른 값을 가짐
- age: 작은 값으로 치우쳐져있으며, 왜도값은 0.57
- fnlwgt: 작은 값으로 치우쳐져있으며, 왜도값은 1.40
- education.num: 비교적 균일한 분포를 보이지만, 일부 범위의 경우 빈도가 급격히 낮아진다.
- capital.gain: 작은 값으로 매우 치우쳐져있으며, 왜도값은 11.91
- captial.loss: 작은 값으로 매우 치우쳐져있으며, 왜도값은 4.73
- hours.per.week: 비교적 균일한 분포를 보인다.
- target: 0의 값이 1의 값에 비해 빈도가 높다.


################################연속형 변수 상관관계	
import numpy as np

corr_df = train_numeric.corr()

# 사이즈 조정
sns.set(rc={'figure.figsize':(11.7,8.27)})

# 절반만 표시하기 위한 mask 설정
mask=np.zeros_like(corr_df, dtype=np.bool)
mask[np.triu_indices_from(mask)]=True

ax = sns.heatmap(corr_df,
                 annot=True, # 데이터 값 표시
                 mask=mask, # 마스크 적용 표시
                 cmap='YlOrRd') # 노랑 / 오렌지 / 빨강

plt.xticks(rotation=45)
plt.title('Relationship of cols', fontsize=20)
plt.show() 

#### 연속형 변수 상관관계 해석

Pearson 상관 계수를 구했을때,
- age~hours.per.week: 0.095양의 상관 관계를 보임
- capital.gain~hours.per.week: 0.33 양의 상관 관계

이외의 변수 관계는 상관 계수가 -0.3 ~ +0.3 사이로 상관 관계가 크지 않다.

그렇다면 영향이 많이 미치는 피쳐들의 상관관계 분포를 알아보겠습니다.

산포도 그래프를 통하여 시각화를 해봅시다.

x축을 capital.gain, y축을 hours.per.week로 두어 시각화를 해봅시다.
plt.style.use('ggplot')
plt.figure(figsize=(12, 10))
plt.title('capital gain and working time', fontsize = 30)
sns.scatterplot(x = 'capital.gain',  y= 'hours.per.week', hue= 'target', 
                data= train[train['capital.gain'] > 0]) 
#산포도를 확실하게 차이나도록  시각화 해주기 위하여 capital.gain에서 0값을 제외
x축에서 capital.gain이 커질수록 >50k 가 많은 것을 확인할 수 있고,

y축에서 크게 차이나보이진 않지만 대체적으로 hours.per.week가 적은쪽에 분포가 <=50k가 많이 형성되어있음을 볼 수 있습니다.

따라서 근무시간 많고 자본의 이익이 많은 집합이 수입이 많을 것이라고 예측이 됩니다.

또한 capital.gain 10만단위대 극단값이 모두 수입이 >50k 인 것을 확인 할 수 있습니다.

다음으로 x축을 age, y축을 capital.loss로 두어 시각화를 해봅시다.

#산포도를 확실하게 차이나도록  시각화 해주기 위하여 capital.loss에서 0값을 제외
plt.style.use('ggplot')
plt.figure(figsize=(12, 10))
plt.title('capital gain and working time', fontsize = 30)
sns.scatterplot(x = 'age',  y= 'capital.loss', hue= 'target', 
                data= train[train['capital.loss'] > 0])
  
x축에서 age가 25세 미만이거나 65세 이상인 곳의 분포가 <=50k가 많이 형성되어있음을 볼 수 있습니다.

y축에서 capital.loss 적은 곳에 <=50k 가 많은 것을 확인할 수 있고,

따라서 나이가 25세~65세 사이와 자본의 손실이 많은 집합이 수입이 많을 것이라고 예측이 됩니다.

또한 capital.loss가 2000이거나 2000바로 아래에 >50K인 집합이 loss 두드러지게 많이 형성되어 있고,
그 아래 1700 부근에 <=50K가 두드러지게 많이 형성되어 있는 것을 확인할 수 있습니다.

#################################################################

### 이상치 확인

이상값 찾는 방법 

1. Z-Score를 이용한 방법
1. ESD(Extreme Studentized Deviation): 평균으로부터 3 표준편차 떨어진 값들
1. 일반 데이터의 범위를 벗어나는 경우 1: 기하평균-2.5표준편차 < data < 기하평균 + 2.5표준편차
1. 일반 데이터의 범위를 벗어나는 경우 2: Q1-1.5*(IQR; Q3-Q1) < data < Q3+1.5*(IQR; Q3-Q1)
1. Boxplot을 통해 이상값으로 o 표기되는 경우
1. externally studentized residual(외면 스튜던트화 잔차)
1. One Class SVM(이하 OC-SVM) 을 이용한 방법
1. DBSCAN 클러스터링을 통해 타겟값이 -1이 되는 경우
1. Isolation forest을 이용한 이상탐지

plt.figure(figsize=(20,10))
train.plot(kind='box', subplots=True, layout=(2,len(train.columns)//2+1), figsize=(20,10))
plt.show()

#################################################################종속 변수 별로 boxplot 확인  
numerical_feature = [ col for col in train_numeric.columns if train_numeric[col].dtypes != "object"]

# train_numeric.describe()
plt.figure(figsize=(15,15))
n = 1
for col in numerical_feature:
    ax = plt.subplot(3,2,n)
    sns.boxplot(x='target', y=col, data=train_numeric)
    plt.title("target - {}".format(col))
    n += 1

plt.tight_layout()
plt.show()

def detect_outliers(df=None, column=None, weight=1.5):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    IQR_weight = IQR * weight
    
    outlier_idx = df[(df[column] < Q1 - IQR_weight) | (df[column] > Q3 + IQR_weight)].index
    
    return outlier_idx
	
print("이상치 갯수: Q1 - 1.5 * IQR 미만, 또는 Q3 + 1.5 * IQR 초과하는 값")
for column in train[numerical_feature].columns:
    print(column, len(train.loc[detect_outliers(df=train[numerical_feature],
                                                column = column), column]))    
    print(train.loc[detect_outliers(df=train[numerical_feature], column = column), column])   

### 이상치 분석
1. Boxplot을 통해 이상값으로 o 표기되는 경우를 확인 하였고,
2. IQR 을 통한 이상치 분석 결과 Prior_purchases , Discount_offered 가 이상치로 검색 되었지만 이상치로 보이지 않기 때문에 그대로 진행한다.

이상값 처리 방법

1. 절단(trim)하는 방법, 
1. 이상치들을 이상치의 하한값, 상한값으로 변환하는 조정(winsorizing)의 방법이 있다. 	



################################## IQR * 1.5 를 기준으로 경계값 너머의 이상치들을 상한값, 하한값으로 치환
data_new = pd.DataFrame()

for col in numerical_feature:
    iqr = train[col].quantile(0.75) - train[col].quantile(0.25)
    line_down = train[col].quantile(0.25) - iqr*1.5
    line_up = train[col].quantile(0.75) + iqr*1.5    
    winsorized = train[col].clip(line_down,line_up)
    data_new.insert(data_new.shape[1], col, winsorized)
    fig = plt.figure(figsize=(10,6))
    ax1 = fig.add_subplot(1,2,1)
    ax2 = fig.add_subplot(1,2,2)
    
    ax1.boxplot(train[col], labels=[col], sym="bo")
    ax1.set_title('before (%s) '%train[col].shape)
#     ax1.set_ylim(0,100)
    
    ax2.boxplot(winsorized, labels=[col])
    ax2.set_title('after (%s) '%winsorized.shape)

plt.show()
###################################################################
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

# 인자로 입력받은 DataFrame을 복사 한 뒤 ID 컬럼만 삭제하고 복사된 DataFrame 반환
def get_preprocessed_df(df=None):
    df_copy = df.copy()
#     display(df_copy)
    df_copy.drop('ID', axis=1, inplace=True)
    
    df_copy.loc[df_copy['Customer_care_calls'] =='$7', 'Customer_care_calls'] = '7'    
    
    df_copy['Discount'] = 0
    df_copy.loc[df_copy['Discount_offered'] > 10, 'Discount'] = 1

    df_copy['Weight'] = 0
    df_copy.loc[(df_copy['Weight_in_gms'] >2000 ) 
                & (df_copy['Weight_in_gms'] < 4000 ), 'Weight'] = 1

    return df_copy

def get_ohe_hotEncoder(v_train, v_test):
    category = [ col for col in v_train.columns if v_train[col].dtypes == "object"]

    ohe = OneHotEncoder(sparse=False,handle_unknown='ignore')
    train_ohe = ohe.fit_transform(v_train[category])

    ohe_columns =[]
    for index, val in enumerate(category):
        for col in ohe.categories_[index]:
            ohe_columns.append(val+'_' + str(col))

    train_ohe_df = pd.DataFrame(train_ohe, columns = ohe_columns ) 
    df_train_ohe_df = pd.concat([v_train.drop(category,axis=1), train_ohe_df], axis =1 )

    test_ohe  = ohe.transform(v_test[category])
    test_ohe_df = pd.DataFrame(test_ohe, columns = ohe_columns )
    df_test_ohe_df = pd.concat([v_test.drop(category,axis=1),test_ohe_df], axis =1 )
    print(df_train_ohe_df.shape, df_test_ohe_df.shape)
    return df_train_ohe_df, df_test_ohe_df
    

# 사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수.
def get_train_test_dataset(v_train, v_test):
    # 인자로 입력된 DataFrame의 사전 데이터 가공이 완료된 복사 DataFrame 반환
    f_train = get_preprocessed_df(v_train)
    f_test = get_preprocessed_df(v_test)
    
    f_train, f_test = get_ohe_hotEncoder(f_train, f_test)
    
    # DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처들
    X_features = f_train.drop('Reached.on.Time_Y.N', axis = 1 )
    y_target = f_train['Reached.on.Time_Y.N']
    
#     stand = StandardScaler()
#     X_features = pd.DataFrame(stand.fit_transform(X_features), columns=X_features.columns)
#     f_test = pd.DataFrame(stand.transform(f_test), columns=f_test.columns)
    

    # train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할
    train_X, test_X, train_y, test_y = \
    train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)
    # 학습과 테스트 데이터 세트 반환
    return train_X, test_X, train_y, test_y


train_X, test_X, train_y, test_y = get_train_test_dataset(train,X_test)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

# 전처리 할것을 넣는다.
def get_preprocessed_df(df=None):
    df_copy = df.copy()  
    return df_copy

def get_train_test_dataset(df, target):
    
    df_copy = get_preprocessed_df(df)
    
    X_features = df_copy.drop(target,axis = 1)
    y_target = df_copy[target]
    
    X_train, X_test, y_train, y_test = train_test_split(X_features,y_target, \
                        test_size=0.3, random_state=0, stratify=y_target)
    
    return X_train, X_test, y_train, y_test

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import log_loss 

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    logscore = log_loss(y_test, pred_proba)
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba[:, 1])    
    print('오차 행렬')
    
    class_names= [0,1] # name  of classes     
    fig, ax =plt.subplots(figsize=(7,7))
    tick_marks = np.arange(len(class_names))    
    plt.xticks(tick_marks, class_names)
    plt.yticks(tick_marks, class_names)
    sns.heatmap(pd.DataFrame(confusion), annot=True , cmap= "YlGnBu" ,fmt= 'g') 
    ax.xaxis.set_label_position("top")
    plt.tight_layout()
    plt.title('Confusion matrix', y= 1.1)
    plt.ylabel('Actual label')
    plt.xlabel('Predicted label')
    plt.show()
    
#     print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}, log_loss:{5:.4f}'.format(accuracy, precision, recall, f1, roc_auc, logscore))
    
# 인자로 사이킷런의 Estimator객체와, 학습/테스트 데이터 세트를 입력 받아서 학습/예측/평가 수행.
def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):
    import time
    start =     time. time()
    model.fit(ftr_train, tgt_train)
    pred = model.predict(ftr_test)
    print("---" * 30)
    print("model:", model.__class__.__name__)
    pred_proba = model.predict_proba(ftr_test)
    print("속도 : " ,time. time() -  start)
    get_clf_eval(tgt_test, pred, pred_proba)    
    print("---" * 30)
	
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

lr = LogisticRegression(max_iter=5000)
get_model_train_eval(lr,train_X,test_X, train_y, test_y)	



####################################################위에서 오버샘플링 한 데이터 2개, 오버샘플링 하기 전 데이터 1개에 대해 모델 2개를 적용하고 성능 보여주기
import time
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import MinMaxScaler


# 불필요 컬럼제거 및 스케일링

if 'date' in X_train.columns:
    X_train = X_train.drop(columns=['date'])
    
if 'date' in X_test.columns:
    X_test = X_test.drop(columns=['date'])


result_auc_train = []
result_auc_test = []
result_time = []
for train_X,trainy in [(X_train,y_train),(X_samp, y_samp),(SMOTE_X_samp, SMOTE_y_samp)]:
    
    trainX = train_X.copy()
    testX = X_test.copy()
    sc = MinMaxScaler()    
    trainX = sc.fit_transform(trainX)
    testX = sc.transform(testX)
    
    
    lrstart = time.time()
    lr =LogisticRegression()
    lr.fit(trainX,trainy)
    lrend = time.time() - lrstart

    pred_lr = lr.predict(testX)
    auc_lr_train = roc_auc_score(trainy,lr.predict(trainX))
    auc_lr = roc_auc_score(y_test,pred_lr)
    
    rfstart = time.time()
    rf =RandomForestClassifier()
    rf.fit(trainX,trainy)
    rfend = time.time() - rfstart
    
    pred_rf  = rf.predict(testX)
    auc_rf_train  = roc_auc_score(trainy,rf.predict(trainX))
    auc_rf  = roc_auc_score(y_test,pred_rf)
    
    result_auc_test.append([auc_lr,auc_rf])
    result_time.append([lrend,rfend])
    result_auc_train.append([auc_lr_train,auc_rf_train])
	
	
#logistic regression 과 randomforest 분류기를 샘플링방식에 따른 학습시 정확도와 모델 학습 시간에 대해서 평가했다.

print('훈련셋 모델 auc 결과')
result_auc_trains = pd.DataFrame(result_auc_train)
result_auc_trains.index = ['raw','randomSampling','SMOTE']
result_auc_trains.columns = ['logistic','randomforest']
display(result_auc_trains)

print('테스트셋 모델 auc 결과')
result_auc_tests = pd.DataFrame(result_auc_test)
result_auc_tests.index = ['raw','randomSampling','SMOTE']
result_auc_tests.columns = ['logistic','randomforest']
display(result_auc_tests)

print('모델 학습시간 (sec)')
result_times = pd.DataFrame(result_time)
result_times.index = ['raw','randomSampling','SMOTE']
result_times.columns = ['logistic','randomforest']
result_times



### 재현율
재현율(recall)은 실제 양성 클래스에 속한 표본 중에 양성 클래스에 속한다고 출력한 표본의 수의 비율을 뜻한다. 높을수록 좋은 모형이다. FDS(사기 거래를 찾아 내는 시스템)의 경우 실제 사기 거래 중에서 실제 사기 거래라고 예측한 거래의 비율이 된다. TPR(true positive rate) 또는 민감도(sensitivity)라고도 한다.

재현율이란 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율입니다. 

$$\text{recall} = \dfrac{TP}{TP + FN}$$

### 정밀도
정밀도(precision)은 양성 클래스에 속한다고 출력한 샘플 중 실제로 양성 클래스에 속하는 샘플 수의 비율을 말한다. 높을수록 좋은 모형이다. FDS의 경우, 사기 거래라고 예측한 거래 중 실제 사기 거래의 비율이 된다.
 
$$\text{precision} = \dfrac{TP}{TP + FP}$$

### Accuracy(정확도)

정확도(accuracy)는 전체 샘플 중 맞게 예측한 샘플 수의 비율을 뜻한다. 높을수록 좋은 모형이다. 일반적으로 학습에서 최적화 목적함수로 사용된다.
 
$$ \text{accuracy} = \dfrac{TP + TN}{TP + TN + FP + FN} $$

정확도는 가장 직관적으로 모델의 성능을 나타낼 수 있는 평가 지표이다. 하지만, 고려해야하는 것이 있다. 바로 domain의 편중(bias)입니다. 만약 우리가 예측하고자 하는 한달 동안이 특정 기후에 부합하여 비오는 날이 흔치 않다고 생각보면 이 경우에는 해당 data의 domain이 불균형하게되므로 맑은 것을 예측하는 성능은 높지만, 비가 오는 것을 예측하는 성능은 매우 낮을 수 밖에 없습니다. 따라서 이를 보완할 지표가 필요합니다.


### F Score
정밀도와 재현율의 가중조화평균(weight harmonic average)을 F점수(F-score)라고 한다. 정밀도에 주어지는 가중치를 베타(beta)라고 한다.
 
$$
F_\beta = (1 + \beta^2) \, ({\text{precision} \times \text{recall}}) \, / \, ({\beta^2 \, \text{precision} + \text{recall}})
$$

베타가 1인 경우를 특별히 F1점수라고 한다.

$$
F_1 = 2 \cdot \text{precision} \cdot \text{recall} \, / \, (\text{precision} + \text{recall})
$$

F1 score는 데이터 label이 불균형 구조일 때, 모델의 성능을 정확하게 평가할 수 있으며, 성능을 하나의 숫자로 표현할 수 있습니다. 

###############################################################
### 이익도표(Lift Chart)


`이익도표 개념`

-이익도표는 분류모형의 성능을 평가하기 위한 척도로, 분류된 관측치에 대해 얼마나 예측이 잘 이루어졌는지 나타내기 위해 임의로 나눈 각 등급별로 반응검츌율, 반응률, 리프트 등의 정보를 산출하여 나타내는 도표이다.

-관심대상(응답고객, 이탈고객 등)을 랜덤하게 확인할 수 있는 것과 비교하여, 모형을 사용했을 대 얼마나 이익을 볼 수 있는지를 비율로 확인

-리프트 도표는 항상 x값이 100%일 때는 1이 된다. 즉 데이터를 모두 추출한다면 굳이 모형을 사용할 필요가 없음

 좋은 모델이라면 Lift가 빠른 속도로 감소해야 한다. 

-> 즉 상위 등급에서 최대한 많이 참값을 걸러내는 모형이 좋은 모형!

***

데이터셋의 각 데이터는 각각 예측 확률을 가진다.

전체 데이터를 예측 확률을 기준으로 내림차순 정렬한다.
 

- 전체 5000명 중에 950명이 실제로 구매

Baseline Lift = 950 / 5000 = 0.19 = 19 %

 

- 예측 확률 상위 10% 500명 중 435명 구매

반응률(Response) = 435 / 500 = 87 %

반응검출률(Captured Response) = 435 / 950 = 45.79 %

 

- 예측 확률 상위 10%의 Lift

Lift = Response / Baseline lift = 87 / 19 = 4.58

좋은 모델이라면 Lift 가 빠른 속도록 감소해야 한다.

* 전체 5000 명을 10개 구간으로 500명씩 구분

# liftchart plotting 함수 정의하기
def liftchart(clf, X_train, y_train, X_test, y_test):
    import warnings
    warnings.filterwarnings('ignore')
    from pandas import DataFrame

    prob = clf.predict_proba(X_test)[:,1]
    actual_y = y_test

    # 예측된 확률과 실제 클래스 데이터프래임을 확률 내림차순으로 정렬
    rank = DataFrame({"pred_prob":prob, 'actual_y':actual_y})\
        .sort_values(by='pred_prob', ascending=False).reset_index(drop=True)

    # 10개 구간으로 나눔
    rank['Decile'] = 10
    start=0
    end = len(rank)//10
    end_start=end-start
    decile = 1
    while end < len(rank):
        for i in range(start, end):
            rank['Decile'][i] = decile
        decile += 1
        start = end
        end += len(rank)//10

    # baseline lift 계산 및 실구매자수 집계
    total = len(X_train) #전체 데이터 수
    count = y_train.sum() #1(True)의 개수
    baseline_lift = count/total

    #print("baseline_lift:", baseline_lift)
    liftchart = rank.groupby('Decile').sum()

    # captured response, response, lift 추가
    liftchart['captured_R'] = liftchart['actual_y']/count
    liftchart['R'] = liftchart['actual_y']/total/10 #10=등급수
    liftchart['lift'] = liftchart['R']/baseline_lift

    from matplotlib import pyplot as plt
    plt.bar(liftchart.index, liftchart['lift'])
    plt.title(clf)
    plt.ylabel("lift")
    plt.xlabel('decile')
    return plt.show()
	
from pandas import read_csv
X_train = read_csv('./X_train.csv')
y_train = read_csv('./y_train.csv')

train = X_train.copy()
train['Reached.on.Time_Y.N'] = y_train['Reached.on.Time_Y.N']
train_df = get_preprocessed_df(train)

train_df = pd.get_dummies(train_df)
X = train_df.drop('Reached.on.Time_Y.N', axis = 1)
y = train_df['Reached.on.Time_Y.N']
X_train, X_test , y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=123)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train,y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train,y_train)

for clf in [lr,rfc]:
    liftchart(clf, X_train, y_train, X_test, y_test)	

from sklearn.model_selection import train_test_split,StratifiedKFold
train_df = get_preprocessed_df(train)
test_df = get_preprocessed_df(X_test)
train_df,test_df  = get_ohe_hotEncoder(train_df, test_df)

X = train_df.drop('Reached.on.Time_Y.N', axis = 1)
y = train_df['Reached.on.Time_Y.N']
stf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)

pred_result = np.zeros(test_df.shape[0])

for train_index, val_index in stf.split(X, y):
    train_x , train_y = X.iloc[train_index], y.iloc[train_index]
    test_x, test_y = X.iloc[val_index], y.iloc[val_index]
    rfc = RandomForestClassifier(n_estimators=500)
    rfc.fit(train_x, train_y)
    pred = rfc.predict(test_x)
    score = accuracy_score(test_y, pred)
#     print(score) 
    pred_proba = rfc.predict_proba(test_df)[:, 1] / stf.n_splits
    pred_result += pred_proba


lgbm_pred = np.zeros((test_df.shape[0]))
for tr_idx, val_idx in stf.split(X, y) :
    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]
    val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]
    
    lgbm = LGBMClassifier(random_state = 42)
    lgbm.fit(tr_x, tr_y)
    val_pred = lgbm.predict_proba(val_x)[:, 1]
    val_pred = [1 if p >= 0.5 else 0 for p in val_pred]
    val_acc = accuracy_score(val_y, val_pred)
    print(val_acc)
    
    fold_pred = lgbm.predict_proba(test_df)[:, 1] / stf.n_splits
    lgbm_pred += fold_pred
	
X_test['target'] = (pred_result + lgbm_pred) / 2
X_test['target'] = [1 if p >= 0.5 else 0 for p in X_test['target']]

y_test = pd.read_csv('./data/test_label/y_test.csv')
accuracy_score(y_test['Reached.on.Time_Y.N'], X_test['target'])	

###############################################################


#### 오버샘플링 적용후 결과 해석 

로지스틱 회귀 모델의 경우 SMOTE로 오버 샘플링된 데이터로 학습할 경우 재현율이 87%로 크게 증가하였지만 반대로 정밀도가 7% 급격하게 저하되었다. 재현율이 높더라도 이정도로 저조한 정밀도로는 현실업무에 적용할수가 없다.  

이는 로지스틱 회귀 모델이 오버 샘플링으로 인해 실제 원본 데이터의 유형보다 너무나 많은 Class =1 데이터를 학습 하면서 실제 테스트 데이터 세트에서 예측을 지나치게 Class = 1로 적용해 정밀도가 급격히 떨어지게 된것이다. 분류 결정 임곗값에 따른 정밀도와 재현율 곡선을 통해서 smote로 학습된 로지스틱 회귀모델에 어떠한 문제가 발생하는지 시각적으로 확인해보자

<font size="5" color="red">재현율이 높다는것은 Class = 1 데이터를 학습 하면서 실제 테스트 데이터 세트 에서도 예측을 지나치게 Class = 1로 적용했다. </font>

아래 precision_recall_curve_plot를 보면 임계값이 0.99이하에서 재현율이 매우 좋고 정밀도가 극단적으로 낮다가 0.99이상에서는 반대로 재현율이 대폭 떨어지고 정밀도가 높아진다. 분류 결정 임계값을 조정하더라도 임계값의 민감도가 너무 심해 올바른 재현율/정밀도 성능을 얻을수 없으므로 로지스틱 회귀모델의 경우 올바른 예측 모델이 생성되지 못했다.

<font size="5" color="red">SMOTE를 적용하면 재현율을 높아지나, 정밀도는 낮아지는것이 일반적이다. 좋은 SMOTE 패키지 일수록 재현율 증가율은 높이고 정밀도 감소율은 낮출수 있도록 효과적으로 데이터를 증식한다.</font>


from sklearn.metrics import precision_recall_curve
def precision_recall_curve_plot(y_test , pred_proba_c1):
    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. 
    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)
    
    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시
    plt.figure(figsize=(8,6))
    threshold_boundary = thresholds.shape[0]
    plt.plot(thresholds, precisions[0:threshold_boundary], 
             linestyle='--', label='precision')
    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')
    
    # threshold 값 X 축의 Scale을 0.1 단위로 변경
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1),2))
    
    # x축, y축 label과 legend, 그리고 grid 설정
    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')
    plt.legend(); plt.grid()
    plt.show()
	
<font size="5">정밀도와 재현율</font>


<font size="4">재현율(recall)/민감도</font>의 FN을 낮춘다는 것은

"Negative라고 잘못 예측하지 않도록 하겠다" = "실제 Positive"를 잘 분리해야 한다. 

즉, "실제 Positive"를 잘못 판단하면 문제가 생기는 경우에 사용하는 지표이다.

예를들어 코로나 환자를 판별한다고 하자. 이때 실제 코로나 환자를 양성(Positive)이라고 잘 분리해 내야 한다. 실제코로나 환자를 음성(Negative)라고 잘못 예측하면 큰일이다. 이는 보험사기, 금융사기 또한 마찬가지다.

<font size="4"> <b>모든 양성 샘플을 식별해야 할때 성능지표 / 거짓 음성을 피하는게 중요/ 민감도 , 적중률, 진짜 양성비율(TPR) 이라고 함</b></font> 

<font size="4">정밀도(precision)</font> 의 FP를 낮춘다는 것은

"Positive라고 잘못 예측하지 않도록 하겠다" = "실제Negative"를 잘 분리해야 한다.

즉, "실제 Negative"를 잘못 판단하면 문제가 생기는 경우에 사용하는 지표이다. 

<font size="4"> <b>정밀도는 거짓 양성(FP)의 수를 줄이는 것이 목표일때 성능 지표/ 양성 예측도다</b></font>

예를들어 스팸메일을 분류한다고 하자. 이때 실제 스팸(Positive)을 잘 걸러내는 것보다는 정상적인 메일(Negative)을 스팸으로 분리하면 문제가 생긴다. 왜? 스팸메일함에 들어간 정상메일은 아에 읽지를 못하기 때문이다. 

==> 정밀도 재현율를 요약을 하면 

재현율이 중요 지표인 경우는 실제 Positive 양성 데이터를 Nagative로 잘못 판단 하게 되면 업무상 큰 영향이 발생 하는 경우 이다. 예를들어서 암 판단 모델은 재현율이 훨씬 중요한 지표 이다. 왜냐하면 실제 Positive 인 암환자를 Positive양성이 아닌 Negative 음성으로 잘못 판단 했을 경우 오류의 대가가 생명을 앗아갈 정도로 심각하기 때문이다. 반면에 실제  Negaitve인 건강한 환잔을 암 환자인 Positive로 예측한 경우면 다시 한번 재검사를 하는 수준의 비용이 소모될것이다.

보험 사기와 같은 금융 사기 적발 모델로 재현율이 중요하다. 실제 금융거래 사기인 Positive 건을 Negative로 잘못 판단 하게 되면 회사에 미치는 손해가 클것이다. 반면에 정상 금융거래인 Negative를 금융사기인 Positive로 잘못 판단하더라도 다시 한번 금융 사기인지 재확인 하는 절자를 가동하면 된다. 물론 고객에게 금융사기 혐의를 잘못 씌우면 문제가 될수 있기에 정밀도도 중요평가 지표지만, 업무적인 특성을 고려하면 재현율이 상대적으로 더 중요한 지표이다.

정밀도가 더 중요한 지표인 경우는 스팸메일 여부를 판단하는 모델의 경우 실제 Positive인 스팸메일을 Negative인 일반메일로 분류하더라도 사용자가 불편함을 느끼는 정도 이지만, 실제 Negative인 일반 메일을 Positive인 스팸메일로 분류할 경우에는 메일을 아예받지 못하게 돼 업무에 차질이 생긴다.

- 재현율이 상대적으로 더 중요한 지표인 경우는 실제 Positive 양성인 데이터예측을 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우
- 정밀도가 상대적으로 더 중요한 지표인 경우는 실제 Negative음성인 데이터 예측을 Positive 양성으로 잘못 판단하게 되면 업무상 큰 영향이 발생하는경우

그렇다고 정밀도와 재현율 중 하나만 스코어가 좋고 다른 하나는 스코어가 나쁜 분류는 성능이 좋지 않는 분류로 간주 할 수 있다. 물론 예제와 같이 분류가 정밀도 또는 재현율중 하나에 상대적인 중요도를 부여해 각 예측 상황에 맞는 분류 알고리즘을 튜닝 할 수 있지만, 그렇다고 정밀도/재현율 중 하나만 강조하는 상황이 돼서는 안된다. (예를들어 암 예측 모델에서 재현율을 높인다고 걸핏하면 양성으로 판단할 경우 환자의 부담과 불평이 커지게 된다.)

정밀도와 재현율의 수치가 적적할게 조합돼 분류의 종합적인 성능 평가에 사용될 수 있는 평가 지표가 필요하다.	


#############################################
#### 사기 탐지 데이터 모델 해석
사기 탐지 같은 경우는 재현율 지표가 더 중요한 지표라고 할 수 있다. 

재현율이 중요 지표인 경우는 실제 Positive 양성 데이터를 Nagative로 잘못 판단 하게 되면 업무상 큰 영향이 발생 하는 경우 이기 때문이다. 

본 예제와 같이 신용카드 사기 탐지 같은 경우는 재현율이 훨씬 중요한 지표 이다. 

실제 사기(Class 1)인 경우를 정상 트랜잭션(Class 0)으로 잘못 판단 하였을 경우 회사 또는 고객에게 피해가 발생 하기 때문이다. 

반면에 실제 Negative인 경우 Positive로 예측한 경우라면 다시 한번 검사 하면 되는 비용만 소모 될것이기 때문이다. 

##########

***
그리고 오버 샘플링을 하는경우는 당연이 클래스 1인 데이터로 증가 시키기 때문에 예측을 1로 하는 경우가 많이 발생 한다..

그렇기 때문에 재현율이 증가 하는것이고...

==> 이말은 Threshold value를 낮추면 클래스 1로 예측을 하려고 하기때문에.. 이부분도 재현율이 증가 한다..


반대로 클래스가 불균형이 심해서 클래스가 1이 별로 없으면 당연히 모델은 클래스 0으로 예측을 많이 하려고 하기 때문에 정밀도가 높아진다.

==> Threshold value를 높이면 모델은 클래스 0으로 예측을 많이 하려고 하기 때문에 정밀도가 높아진다.


# UDF for contribution(sensitivity) analysis per each variables
# task: "LinearReg" or "LogitReg"
def sensitivity_analysis_LinearReg_LogitReg(task, model, X, idx, bar_plot_yn):
    
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import statsmodels.api as sm
    pd.options.mode.chained_assignment = None
    
    # get one object's X values
    X_i = X.iloc[idx, :]
    
    # make a matrix with zeros with shape of [num_cols, num_cols]
    X_mat = np.zeros(shape=[X_i.shape[0], X_i.shape[0]])
    
    # fil X_mat with values from one by one columns, leaving the ohters zeros
    for i, j in enumerate(X_i):
        X_mat[i, i] = j
        
    # data frame with contribution of each X columns in descending order
    sensitivity_df = pd.DataFrame({
        'idx': idx
        , 'task': task
        , 'x': X_i
        , 'contribution_x': model.predict(X_mat)     
    })
    
#     # ==== Remark =====
#     # if you used LogisticRegressionsklearn from sklearn.linear_model
#     # then use codes below
#     if task == "LinearReg":
#         sensitivity_df = pd.DataFrame({
#             'idx': idx
#             , 'task': task
#             , 'x': X_i
#             , 'contribution_x': model.predict(X_mat) 
#         })
        
#     elif task == "LogitReg":
#         sensitivity_df = pd.DataFrame({
#             'idx': idx
#             , 'task': task
#             , 'x': X_i
#             , 'contribution_x': model.predict_proba(X_mat)[:,1] 
#         })
#     else:
#         print('Please choose task one of "LinearReg" or "LogitReg"...')
    
    
    sensitivity_df = sensitivity_df.sort_values(by='contribution_x', ascending=True)
    
    # if bar_plot_yn == True then display it
    col_n = X_i.shape[0]
    
    if bar_plot_yn == True:
        sensitivity_df['contribution_x'].plot(kind='barh', figsize=(10, 0.7*col_n))
        plt.title('Sensitivity Analysis', fontsize=18)
        plt.xlabel('Contribution', fontsize=16)
        plt.ylabel('Variable', fontsize=16)
        plt.yticks(fontsize=14)
        plt.show()
    
    return sensitivity_df.sort_values(by='contribution_x', ascending=False)
# apply sensitivity analysis function on 1st observation for Logistic Regression

sensitivity_analysis_LinearReg_LogitReg(task="LogitReg"
                                        , model=logitreg_fit
                                        , X=X_test
                                        , idx=0
                                        , bar_plot_yn=True)



## #5 추가적인 개선 방안(계속 해서 정리해야 함====>)

- 모델 학습의 X_train, y_train에서 0과 1의 값이 균형적인 분포가 보이도록, 오버샘플링, 특히 SMOTE와 같은 방법으로 오버샘플링을 실시할 경우, 데이터를 보존하면서, 과대적합을 방지할 수 있고 이로 인해, f1_score, roc_auc_socre가 향상될 수 있다.
- X_train의 변수의 왜도의 크기가 큰 변수들의 경우 box-cox, 로그 정규화 등 분포를 정규화 시킬 수 있다면, 모델의 성능이 향상될 수 있다.
- rfc, xgb의 매개변수를 조절하여 grid_search 등과 같은 방법으로 적합한 매개변수를 찾는다면, 모델의 성능이 향상될 수 있다.

##################################################
세 가지 모델 모두 모델링 해보고 가장 적합한 알고리즘 선택하고 이유 설명. 한계점 설명하고 보완 가능한 부분 설명

from sklearn.svm import SVR 
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

from sklearn.svm import SVC

scaler = StandardScaler() 
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)


from sklearn.model_selection import GridSearchCV

param_grid = [
    { 'C': [0.1, 1,10,100],'gamma': [0.001, 0.01, 0.1, 1, 10]}
]

grid_svm = GridSearchCV(SVR(), param_grid =param_grid, cv = 5)
grid_svm.fit(X_train_scaled, y_train)

result = pd.DataFrame(grid_svm.cv_results_['params'])
result['mean_test_score'] = grid_svm.cv_results_['mean_test_score']
result.sort_values(by='mean_test_score', ascending=False)

svr = SVR(C=100, gamma = 0.001) 
svr.fit(X_train_scaled, y_train)

print("R2 : ", svr.score(X_test_scaled, y_test))
print("RMSE:", np.sqrt(mean_squared_error(y_test,svr.predict(X_test_scaled))))

rf_grid = [
    { 'max_depth': [2,4,6,8,10], 'min_samples_split': [2, 4, 6, 8, 10]}
]

rf  = GridSearchCV(RandomForestRegressor(n_estimators=100), param_grid =rf_grid, cv = 5)
rf.fit(X_train, y_train)

print(rf.best_params_)
print("R2 : ", rf.score(X_test, y_test))
print("RMSE:", np.sqrt(mean_squared_error(y_test,rf.predict(X_test))))

xgb_grid = [
    { 'max_depth': [2,4,6,8,10]}
]

xgb = GridSearchCV(XGBRegressor(n_estimators=1000), param_grid =xgb_grid, cv = 5)
xgb.fit(X_train, y_train)
xgb.score(X_test, y_test)

print("R2 : ", xgb.score(X_test, y_test))
print("RMSE:", np.sqrt(mean_squared_error(y_test,xgb.predict(X_test))))

from xgboost import plot_importance
plot_importance(xgb)

## 해석

- 가장적합한 알고리즘 선택 : XGBoost 모델이 가장 정확도가 높고 RMSE값이 낮으므로 예측 분석력이 좋다. 
- XGBoost 분석결과 가족친밀도인 farmrel변수가 최종 성적에 영향을 많이 주는것으로 보였다. 
- 한계점 : G1,G2가 grade와의 상관성이 매우 높은 변수이므로 좋은 결과가 나오지만, 해당 변수를 제외하고 분석하였을 때에는 정확도가 매우 낮게 나온다. 주 변수로 G1, G2를 사용할 수는 있지만 이 또한 성적이기 때문에 성적에 영향을 미치는 변수를 찾기에는 어려워보인다. 성적과 밀접 관련이 있는 추가적인 변수를 추가한다면, 모델의 정확성 뿐만이 아니라 의미있는 분석이 될 것이라 생각한다. 







	