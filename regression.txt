#회귀분석

plt.rc("font", family = "Malgun Gothic")
import matplotlib
matplotlib.rcParams['axes.unicode_minus'] = False

숫자형 자료들의 기본적인 통계 자료들을 파악
df_train.describe()

문자형 자료들의 기본 정보를 파악
# int, float 을 제외한 object만 문자형 자료다.
df_train.describe(include='object')

중복 확인
# Checking duplicates
print(sum(df_train.duplicated(subset = 'Id')) == 0)

### 데이터 구조 Insight
- 1460개의 행을 가지고, 81개의 열을 가지는 데이터
- 8개의 범주형 데이터와 7개의 수치형 데이터, 1개의 이진형 데이터가 존재한다.
- 결측치는 workclass의 경우 1836개, occupation의 경우 1843개 native.country의 경우 583개를 가짐
- ID를 기준으로 중복된 데이터는 존재 하지 않는다.

## 결측치 확인
def check_missing_col(dataframe):
    missing_col = []
    for col in dataframe.columns:
        missing_values = sum(dataframe[col].isna())
        is_missing = True if missing_values >= 1 else False
        if is_missing:
            print(f'결측치가 있는 컬럼은: {col} 입니다')
            print(f'해당 컬럼에 총 {missing_values} 개의 결측치가 존재합니다.')
            missing_col.append([col, dataframe[col].dtype,missing_values])
    if missing_col == []:
        print('결측치가 존재하지 않습니다')
    return missing_col

missing_col = check_missing_col(df_train)
pd.DataFrame(missing_col)


# 결측치가 있는 row들을 확인합니다.
df_train[df_train.isna().sum(axis=1) > 0]

for col in missing_col:    
    print("피쳐 " , col[0])
    print(df_train[col[0]].unique())
    
#결측치 시각화
missing = df_train.isnull().sum()
missing = missing[missing > 0]
missing.sort_values(inplace=True)

missing.plot.bar(figsize = (12,6))    

plt.xlabel("", fontsize = 20)
plt.ylabel("", fontsize = 20)
plt.title("Total Missing Value", fontsize = 20)

for i, val in enumerate(missing.values):
    plt.text(i,val,"%s"%val, horizontalalignment='center')    
plt.show()

#결측치 상관관계
import seaborn as sns
import matplotlib.pyplot as plt
missingdata_df  = df_train.columns[df_train.isnull().any()].tolist()
colormap = plt.cm.PuBu
sns.heatmap(df_train[missingdata_df].corr(),square = True, linewidths = 0.1,
            cmap = colormap, linecolor = "white", vmax=0.8,annot=True, )
plt.title("Correlation with Missing Values", fontsize = 20)
plt.show()

#종속변수 시각화
f , axes = plt.subplots(1,4)
axes = axes.flatten()
f.set_size_inches(20,5)

# 이적료에 log
df_train["log_SalePrice"] = np.log(df_train['SalePrice'])

sns.histplot(x="SalePrice", data=df_train, bins=20,ax=axes[0],
             kde=True,stat="density")
axes[0].set(title = "SalePrice")
sns.histplot(x="log_SalePrice", data=df_train, ax=axes[1])
axes[1].set(title = "log_SalePrice")
sns.boxplot(y="SalePrice", data=df_train, ax=axes[2])
axes[2].set(title = "SalePrice")
sns.boxplot(y="log_SalePrice", data=df_train, ax=axes[3])
axes[3].set(title = "log_SalePrice")

import scipy.stats as stats

stats.shapiro(df_train['SalePrice'])

# 범주형 변수별 그룹확인
from IPython.display import display
cate_feat = []
num_feat = []
for col in df_train.columns:
    target = df_train[col]
    if target.nunique() <=50:
        print(col,df_train[col].dtype,target.unique())
        display(target.value_counts().to_frame())
#         print()
        cate_feat.append(col)
    else:
        print('연속형', col, df_train[col].dtype,len(target.unique()))
        num_feat.append(col)
print('범주형 :', cate_feat)
print('연속형: ', num_feat)

#범주형 변수 데이터 시각화
plt.figure(figsize=(20,70)) # 먼저 창을 만들고
n = 1
for col in cate_feat:
    unique_df = df_train[col].value_counts()
    
    if len(unique_df) < 150:
        ax = plt.subplot(31,2,n) # for문을 돌면서 Axes를 추가
        unique_df.plot(kind='bar') 
        plt.title(col) 
        n+=1

# plt.tight_layout()  # 창 크기에 맞게 조정        
plt.show()         

# 범주형 변수 상관관계
from scipy.stats import chi2_contingency, chisquare
import seaborn as sns 

category_feature = cate_feat.copy()
for col in ['PoolQC' , 'MiscFeature', 'Alley', 'Fence','FireplaceQu']:
    category_feature.remove(col)

corr_list= []

for i in category_feature:
    c_list = []
    for j in category_feature:
        ct = pd.crosstab(df_train[i],df_train[j])
        X2=chi2_contingency(observed=ct)[0]
        n = len(df_train)
        minDim = min(len(df_train[i].unique()),len(df_train[j].unique()))-1
        c = np.sqrt((X2/n) / minDim)
#         c = np.sqrt(result[0]/(len(train)*(min(len(train[i].unique()),len(train[j].unique()))-1)))
        c_list.append(c)
    corr_list.append(c_list)
    
corr_df = pd.DataFrame(corr_list,columns=category_feature, index=category_feature)

sns.set(rc = {'figure.figsize':(20,12)})
sns.heatmap(corr_df,vmin=-1,vmax=1,cmap='RdBu',linewidths=.1,annot=True, fmt='.2f')

#연속형 변수 시각화
import warnings
warnings.filterwarnings(action='ignore')
n = 1

plt.figure(figsize=(20,70)) # 먼저 창을 만들고
for col in num_feat:
    ax = plt.subplot(22,2,n)
    sns.distplot(df_train.loc[df_train[col].notnull(), col])
    plt.title(col)
    n += 1
    
plt.tight_layout()  # 창 크기에 맞게 조정         
plt.show()

# 연속형 자료 boxplot
import warnings
warnings.filterwarnings(action='ignore')
n = 1

plt.figure(figsize=(20,70)) # 먼저 창을 만들고
for col in num_feat:
    ax = plt.subplot(22,2,n)    
    sns.boxplot(data = df_train, y=col, ax=ax)
    plt.title(col)
    n += 1
    
plt.tight_layout()  # 창 크기에 맞게 조정         
plt.show()

#범주형 변수 및 종속변수 boxplot
li_cat_feats = list(cate_feat)
nr_rows = 15
nr_cols = 3

fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))

for r in range(0,nr_rows):
    for c in range(0,nr_cols):  
        i = r*nr_cols+c
        if i < len(li_cat_feats):
            sns.boxplot(x=li_cat_feats[i], y=df_train["SalePrice"], 
                        data=df_train, ax = axs[r][c])
    
plt.tight_layout()    
plt.show()

#연속형 변수 상관관계

import numpy as np

corr_df = df_train.corr()

# 사이즈 조정
sns.set(rc={'figure.figsize':(25,12)})

# 절반만 표시하기 위한 mask 설정
mask=np.zeros_like(corr_df, dtype=np.bool)
mask[np.triu_indices_from(mask)]=True

# ax = sns.heatmap(corr_df,
#                  annot=True, # 데이터 값 표시
#                  mask=mask, # 마스크 적용 표시
#                  cmap=plt.cm.PuBu)

sns.heatmap(df_train.corr(),square = True, linewidths = 0.1,
            cmap = colormap, linecolor = "white", vmax=0.8,annot=True)

plt.xticks(rotation=45)
plt.title('Relationship of cols', fontsize=20)
plt.show() 

#변수가 많을 경우 종속변수와 상관계수가 높은 순으로 히트맵 그려보자
k= 11
cols = df_train.corr().nlargest(k,'SalePrice')['SalePrice'].index
print(cols)
cm = np.corrcoef(df_train[cols].values.T)
f , ax = plt.subplots(figsize = (12,10))
sns.heatmap(cm, vmax=.8, linewidths=0.1,square=True,annot=True,cmap=colormap,
            linecolor="white",xticklabels = cols.values ,
            annot_kws = {'size':14},yticklabels = cols.values)


# null 처리
# Null 이 너무 많은 컬럼들과 불필요한 컬럼 삭제
df_train = df_train.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', 
                          'Fence','FireplaceQu'], axis=1 )
df_test = df_test.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', 
                        'Fence','FireplaceQu'], axis=1 )

# Drop 하지 않는 숫자형 Null컬럼들은 평균값으로 대체
df_train.fillna(df_train.mean(),inplace=True)
df_test.fillna(df_test.mean(),inplace=True)

# Null 값이 있는 피처명과 타입을 추출
null_column_count = df_train.isnull().sum()[df_train.isnull().sum() > 0]
print('## Null 피처의 Type :\n', df_train.dtypes[null_column_count.index])
'''
문자형 피처를 제외 하고는 null 값이 없다. 문자형 피처는 원핫 인코딩으로 변환..
원한 인코딩으로 변화를 하면 null 값은 자동으로 'None' 칼럼으로 대체 해주기 때문에 
별도의 null 처리가 필요없다. 
'''


from sklearn.metrics import mean_squared_error, mean_absolute_error,\
                mean_absolute_percentage_error, r2_score

# log 값 변환 시 NaN등의 이슈로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산
def rmsle(y, pred):
    log_y = np.log1p(y)
    log_pred = np.log1p(pred)
    squared_error = (log_y - log_pred) ** 2
    rmsle = np.sqrt(np.mean(squared_error))
    return rmsle

# 사이킷런의 mean_square_error() 를 이용하여 RMSE 계산
def rmse(y,pred):
    return np.sqrt(mean_squared_error(y,pred))

# MSE, RMSE, RMSLE 를 모두 계산 
def evaluate_regr(y,pred):
    rmsle_val = rmsle(y,pred)
    rmse_val = rmse(y,pred)
    # MAE 는 scikit learn의 mean_absolute_error() 로 계산
    mae_val = mean_absolute_error(y,pred)
    mse = mean_squared_error(y,pred)
    mape = mean_absolute_percentage_error(y,pred)
    r2 = r2_score(y,pred)
    print('RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.
                              format(rmsle_val, rmse_val, mae_val))
    print('MSE :{0:.3F}, MAPE :{1:.3F}, R2 :{2:.3F}  '.
                              format(mse, mape, r2))
    
# 모델과 학습/테스트 데이터 셋을 입력하면 성능 평가 수치를 반환
def get_model_predict(model, X_train, X_test, y_train, 
                                  y_test, is_expm1=False):
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    if is_expm1 :
        y_test = np.expm1(y_test)
        pred = np.expm1(pred)
    print('###',model.__class__.__name__,'###')
    evaluate_regr(y_test, pred)
# end of function get_model_predict      

def get_top_error_data(y_test, pred, n_tops = 5):
    # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. 
    result_df = pd.DataFrame(y_test.values, columns=['real_count'])
    result_df['predicted_count']= np.round(pred)
    result_df['diff'] = np.abs(result_df['real_count'] - 
                                   result_df['predicted_count'])
    # 예측값과 실제값이 가장 큰 데이터 순으로 출력. 
    print(result_df.sort_values('diff', ascending=False)[:n_tops])
    
def get_top_bottom_coef(model, n=10):
    # coef_ 속성을 기반으로 Series 객체를 생성. index는 컬럼명. 
    coef = pd.Series(model.coef_, index=X_features.columns)
    
    # + 상위 10개 , - 하위 10개 coefficient 추출하여 반환.
    coef_high = coef.sort_values(ascending=False).head(n)
    coef_low = coef.sort_values(ascending=False).tail(n)
    return coef_high, coef_low

def visualize_coefficient(models):
    
    # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 회귀 계수 시각화. 
    for i_num, model in enumerate(models):
         # 3개 회귀 모델의 시각화를 위해 3개의 컬럼을 가지는 subplot 생성
        fig, axs = plt.subplots(figsize=(10,8),nrows=1, ncols=1)        
        # 상위 10개, 하위 10개 회귀 계수를 구하고, 이를 판다스 concat으로 결합. 
        coef_high, coef_low = get_top_bottom_coef(model)
#         print(coef_high, coef_low)
        coef_concat = pd.concat( [coef_high , coef_low] )
        # 순차적으로 ax subplot에 barchar로 표현. 한 화면에 표현하기 위해 
        # tick label 위치와 font 크기 조정. 
        axs.set_title(model.__class__.__name__+' Coeffiecents', size=25)
        
        for label in (axs.get_xticklabels() + axs.get_yticklabels()):
            label.set_fontsize(22)
            
        sns.barplot(x=coef_concat.values, y=coef_concat.index , ax=axs)
        
        
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

y_target = df_train_ohe_df['SalePrice']
X_features = df_train_ohe_df.drop(['SalePrice'], axis=1)

X_train, X_test, y_train,y_test = train_test_split(X_features,
                                y_target,test_size=0.3, random_state=123)

lr_reg = LinearRegression()
lr_reg.fit(X_train,y_train)
pred = lr_reg.predict(X_test)

evaluate_regr(y_test,pred)

# 실제값과 예측값이 어느정도 차이가 나는지 확인
get_top_error_data(y_test,pred)

# 종속변수 로그 변환후 학습
y_target_log = np.log1p(y_target)

sns.histplot(y_target_log, bins=20,kde=True,stat="density")
plt.show()

# 전체 데이터 셋으로 교차 검증수행
from sklearn.model_selection import cross_val_score

def get_avg_rmse_cv(models):
    for model in models:
        # 분할하지 않고 전체 데이터로 cross_val_score( ) 수행. 모델별 CV RMSE값과 평균 RMSE 출력
        rmse_list = np.sqrt(-cross_val_score(model, X_features, y_target_log,
                                scoring="neg_mean_squared_error", cv = 5))
        rmse_avg = np.mean(rmse_list)
        print('\n{0} CV RMSE 값 리스트: {1}'.format( 
                                model.__class__.__name__, np.round(rmse_list, 3)))
        print('{0} CV 평균 RMSE 값: {1}'.format( 
                                model.__class__.__name__, np.round(rmse_avg, 3)))

# 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 CV RMSE값 출력           
models = [lr_reg, ridge_reg, lasso_reg]
get_avg_rmse_cv(models)


# 하이퍼 파라미터 튜닝
from sklearn.model_selection import GridSearchCV

def print_best_params(model, params):
    grid_model = GridSearchCV(model, param_grid=params, 
                              scoring='neg_mean_squared_error', cv=5)
    grid_model.fit(X_features, y_target_log)
    rmse = np.sqrt(-1* grid_model.best_score_)
    print('{0} 5 CV 시 최적 평균 RMSE 값: {1}, 최적 alpha:{2}'.format(
                                        model.__class__.__name__,
                                np.round(rmse, 4), grid_model.best_params_))
    return grid_model.best_estimator_

ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }
lasso_params = { 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03,
                                  0.1, 0.5, 1,5, 10] }

best_rige = print_best_params(ridge_reg, ridge_params)
best_lasso = print_best_params(lasso_reg, lasso_params)

#모델 수행
from sklearn.linear_model import Ridge,Lasso

# LinearRegression, Ridge, Lasso 학습, 예측, 평가

lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)

ridge_reg = Ridge(alpha=12)
ridge_reg.fit(X_train, y_train)

lasso_reg = Lasso(alpha=0.001)
lasso_reg.fit(X_train, y_train)

models = [lr_reg, ridge_reg, lasso_reg]

for i_num, model in enumerate(models):
    get_model_predict(model,X_train, X_test, y_train, y_test,True)        
    
    
# Feature 분포도 및 이상치 데이터 처리 후
from scipy.stats import skew

numerical_feature = list(set(df_train.columns) - set(category_feature) - 
                                     set(['Id', 'SalePrice']))

# data[numerical_feature].skew()
skew_features = df_train[numerical_feature].apply(lambda x : skew(x))
'''
skew(왜곡)정도가 1이상인 칼럼만 추출
'''
skew_features_top = skew_features[skew_features > 1]
skew_features_top.sort_values(ascending=False)

from scipy.stats import skew

# data[numerical_feature].skew()
skew_features_test = df_test[numerical_feature].apply(lambda x : skew(x))
'''
skew(왜곡)정도가 1이상인 칼럼만 추출
'''
skew_features_test_top = skew_features_test[skew_features_test > 1]
skew_features_test_top.sort_values(ascending=False)

df_train[skew_features_top.index] = np.log1p(df_train[skew_features_top.index])
df_test[skew_features_test_top.index] = 
                                np.log1p(df_test[skew_features_test_top.index])
    
X_features = df_train_ohe_df.drop(['SalePrice'], axis=1)

X_train, X_test, y_train,y_test = train_test_split(X_features,
                                y_target_log,test_size=0.3, random_state=123)

ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }
lasso_params = { 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03,
                          0.1, 0.5, 1,5, 10] }

best_rige = print_best_params(ridge_reg, ridge_params)
best_lasso = print_best_params(lasso_reg, lasso_params)    


#

from xgboost import XGBRegressor
xgb_param ={'n_estimators':[1000]}

xgb_reg = XGBRegressor(n_estimators =1000,learning_rate =0.05, 
                       colsample_bytree=0.5, subsample=0.8)
best_xgb = print_best_params(xgb_reg,xgb_param)

from lightgbm import LGBMRegressor
lgb_param ={'n_estimators':[1000]}
lgb_reg = LGBMRegressor(n_estimators =1000,learning_rate =0.05,
                        num_leaves=4, colsample_bytree=0.4, 
                        subsample=0.6, reg_lambda=10, n_jobs=-1)
best_lgbm = print_best_params(lgb_reg,lgb_param)

# 모델의 중요도 상위 20개의 피처명과 그때의 중요도값을 Series로 반환.
def get_top_features(model):
    ftr_importances_values = model.feature_importances_
    ftr_importances = pd.Series(ftr_importances_values, index=X_features.columns  )
    ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]
    return ftr_top20

def visualize_ftr_importances(models):
    # 2개 회귀 모델의 시각화를 위해 2개의 컬럼을 가지는 subplot 생성
    fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=2)
    fig.tight_layout() 
    # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 피처 중요도 시각화. 
    for i_num, model in enumerate(models):
        # 중요도 상위 20개의 피처명과 그때의 중요도값 추출 
        ftr_top20 = get_top_features(model)
        axs[i_num].set_title(model.__class__.__name__+' Feature Importances', size=25)
        #font 크기 조정.
        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):
            label.set_fontsize(22)
        sns.barplot(x=ftr_top20.values, y=ftr_top20.index , ax=axs[i_num])

# 앞 예제에서 print_best_params( )가 반환한 GridSearchCV로 최적화된 모델의 피처 중요도 시각화    
models = [best_xgb, best_lgbm]
visualize_ftr_importances(models)


# 회귀 모델의 예측 결과 혼합을 통한 최종 예측
# MSE, RMSE, RMSLE 를 모두 계산 
def evaluate_regr(y,pred, is_expm1=False):
    if is_expm1 :
        y = np.expm1(y)
        pred = np.expm1(pred)
        
    rmsle_val = rmsle(y,pred)
    rmse_val = rmse(y,pred)
    # MAE 는 scikit learn의 mean_absolute_error() 로 계산
    mae_val = mean_absolute_error(y,pred)
    mse = mean_squared_error(y,pred)
    mape = mean_absolute_percentage_error(y,pred)
    r2 = r2_score(y,pred)
    print('RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.
                                    format(rmsle_val, rmse_val, mae_val))
    print('MSE :{0:.3F}, MAPE :{1:.3F}, R2 :{2:.3F}  '.
                                      format(mse, mape, r2))   

# 개별 모델의 학습
ridge_reg = Ridge(alpha=8)
ridge_reg.fit(X_train, y_train)
lasso_reg = Lasso(alpha=0.001)
lasso_reg.fit(X_train, y_train)
# 개별 모델 예측
ridge_pred = ridge_reg.predict(X_test)
lasso_pred = lasso_reg.predict(X_test)

# 개별 모델 예측값 혼합으로 최종 예측값 도출
pred = 0.5 * ridge_pred + 0.5 * lasso_pred
preds = {'최종 혼합': pred,
         'Ridge': ridge_pred,
         'Lasso': lasso_pred}
#최종 혼합 모델, 개별모델의 RMSE 값 출력
evaluate_regr(y_test, pred,True)    


#### 대치 (Imputation)
결측치를 특정 값으로 대치하는 것이다.

1. Mean/Median Imputation
       한 컬럼에 있는 missing value를 결측 되지 않은 다른 값들의 평균(mean)이나 중앙값(median)으로 대체하는 것이다.   
       다른 feature는 고려되지 않는다. 또한 숫자형 데이터에만 사용할 수 있다.
                     
        * 장점 :
            - 쉽고 빠르다
            - 작은 크기의 숫자형 데이터셋에 잘 동작한다.  
        * 단점 :
            - 다른 feature 간의 상관관계가 고려되지 않는다. 단순히 결측지가 존재하는 컬럼만 고려된다.
            - 인코딩 된 범주형 feature에 대해 안 좋은 결과를 제공한다.(절대 범주형 feature에 사용하지 말 것)
            - 정확하지 않다.
            - 불확실성에 반대된다.(동일한 값이므로)  
        
2. Most Frequent Value / Zero / Constant Imputation  
       Most Frequent Value Imputation : 가장 빈번히 나온 값으로 대체한다. 이건 범주형 feature에도 잘 동작한다.  
       Zero Imputation : 말 그대로 0으로 대체한다.  
       Constant Imputation : 지정한 상수값으로 대체한다
       
        * 장점:
            - 이것도 쉽고 빠름
            - categorical(범주형) feature에 잘 동작함
        * 단점:
            - 이것 또한 다른 feature 간의 상관관계가 고려되지 않는다.
            - 데이터에 bias를 만들 수 있다.
    
3. K-NN Imputation      
        K-NN(k nearest neighbours) 이란 classification에 사용되는 간단한 알고리즘이다. 'feature similarity'를 이용해 가장 닮은(근접한) 데이터를 K개를 찾는 방식이다. KNN을 사용하는 간단한 방법은 impyute 라이브러리를 사용하는 것이다. 
        * 장점:
            - mean, median이나 most frequent 보다 정확할 때가 많다.(데이터셋에 따라 다르다.)
        * 단점:
            - 메모리가 많이 필요하다. 전체 데이터 세트를 메모리에 올려야 한다. 계산량도 많다
            - outlier에 민감하다.
            - feature의 scale이 중요하다. (유클리드 or 맨허튼 거리를 기반으로 하기때문에)
            - 고차원 데이터에서 매우 부정확할 수 있다.

4. linear Imputation     
    선형보간법은 두 점 사이에 직선을 그립니다. 그리고 그 선을 이용해서 f(x)를 추정합니다. 두 점의 좌표를 알면 1차 함수식을 만들 수 있죠? y = mx + n에 두 좌표값을 대입시키면 그 1차 함수식을 찾아낼 수 있습니다. 알고싶은 좌표를  지정하면 선형 보간법을 사용하여 점 y 좌표까지 알아낼 수 있다.


5. MICE(Multivariate Imputation by Chained Equation) Imputation 좋은 거 + 좋은 거 = 좋은 거
        이 방식은 누락된 데이터를 여러 번 채우는 방식으로 작동한다. Multiple Implutation(MI)는 불확실성을 고려했을 때 Single Imputation 보다 훨씬 낫다. chained equeation 접근 법은 매우 유연해서 연속형, 이진형, 범위형, survey skip 패턴도 처리할 수 있다.
        

6. Interpolation and Extrapolation(보간법/보외법)
보간법이란, 하나의 추정 방법으로, 실험과 조사로부터 관측된 데이터(x) 사이(중간)의 x값에 대해 함수값을 예측하는 방법입니다.
주어진 관측값들을 바탕으로 근사시킨 함수(f(x))를 이용하여, 직접 조사되지 않은 데이터(주어진 관측값들의 범위 안에 존재해야함) 에 대한 함수값을 예측하는 방법.. 한마디로 두 지점의 사이 값을 추정한다는 말.  
보외법이란 : 지금까지 경향을 보고 밖(미래/과거)의 값을 추정한다는 말.

    아래 예제를 통해 확인

7. Hot-Deck 대체 방법  
해당 변수의 실제 관측된 값 중에서 하나를 추출하여 결측 값에 대체하는 방법.  결측값이 발생 하면 결측값과 동일한 지역이나 성별 등 동일한 특성을 가지고 있는 값들 중에서 하나를 무작위로 추출해서 할당


8. 회귀분석을 이용한 대치
예제를 통해서 확인

- 사용함수 : fillna(), replace(), interpolate()
    * fillna() 
        * 전체 결측치를 특정 단일값으로 대체하기  df.fillna(0)  
        * 특정열에 결측치가 있을 경우 다른 값으로 대체하기  
            * 0으로 대체하기  
                df['col'] = df['col'].fillna(0)  
            * 컬럼의 평균으로 대체하기  
                df['col'] = df['col'].fillna(df['col'].mean())  
            * 결측치 바로 이전 값으로 채우기
                df.fillna(method = 'pad')
            * 결측치 바로 이후 값으로 채우기  
                df.fillna(method = 'bfill')
     * replace() 
        * 결측치 값을 -50으로 채운다.  
          df.replace(to_replace = np.nan, value = -50)
     * interpolate() 
        * interpolate는 인덱스를 무시하고 값들을 선형적으로 같은 간격으로 처리하게 된다.  
          df.interpolate(method = 'linear' , limit_direction = 'forward')  


####################################################################################################################
# 데이터 스케일링

데이터를 모델링하기 전에는 반드시 스케일링 과정을 거쳐야 한다. 스케일링을 통해 다차원의 값들을 비교 분석하기 쉽게 만들어주며, 자료의 오버플로우(overflow)나 언더플로우(underflow)를 방지 하고, 독립 변수의 공분산 행렬의 조건수(condition number)를 감소시켜 최적화 과정에서의 안정성 및 수렴 속도를 향상 시킨다.

* 스케일링을 하는 이유
- 변수의 크기가 너무 작거나, 너무 큰 경우 해당 변수가 target에 미치는 영향력을 제대로 반영하기 어렵기 때문
- 과적합된 모델은 극도로 크거나 작은 가중치를 가지는 경향 보임

데이터를 모델링하기 전에는 반드시 스케일링 과정을 거쳐야 한다. 스케일링을 통해 다차원의 값들을 비교 분석하기 쉽게 만들어주며, 자료의 오버플로우(overflow)나 언더플로우(underflow)를 방지 하고, 독립 변수의 공분산 행렬의 조건수(condition number)를 감소시켜 최적화 과정에서의 안정성 및 수렴 속도를 향상 시킨다. 특히 k-means 등 거리 기반의 모델에서는 스케일링이 매우 중요하다  
변수의 크기가 너무 작거나 크면 해당 변수가 Target에 미치는 영향력이 제대로 표현되지 않을 수 있다  
예를 들어, X1 특성은 0부터 1사이의 소수값을 갖는다 비해, X2 특성은 1000000부터 1000000000000사이의 소수값을 갖는 상황에서 y 값은 1000000부터 100000000까지의 값을 갖는 다고 가정하면, 사실 X1 특성은 y를 예측하는데 큰 영향을 주지 않는 것으로 생각할 수 있습니다.(이 외에도 overflow, underflow, 발산 수렴 등의 문제가 있습니다.) 때문에 이런 상태에서는 머신러닝이 잘 작동하지 않게 됩니다. 


회귀분석에서의 조건수

함수의 조건수(condition number)는 argument에서의 작은 변화의 비율에 대해 함수가 얼마나 변화할 수 있는지 에 대한 argument measure이다.

조건수가 크면 약간의 오차만 있어도 해가 전혀 다른 값을 가진다. 따라서 조건수가 크면 회귀분석을 사용한 예측값도 오차가 커지게 된다.

회귀분석에서 조건수가 커지는 경우는 크게 두 가지가 있다.

1) 변수들의 단위 차이로 인해 숫자의 스케일이 크게 달라지는 경우. 이 경우에는 스케일링(scaling)으로 해결한다.

2) 다중 공선성 즉, 상관관계가 큰 독립 변수들이 있는 경우, 이 경우에는 변수 선택이나 PCA를 사용한 차원 축소 등으로 해결한다.

## 스케일링 종류
1. StandardScaler  
각 특성의 평균을 0, 분산을 1로 스케일링합니다. 즉 데이터를 정규분포로 만듭니다. 하한값과 상한값이 존재하지 않을 수 있기에, 어떤 알고리즘에서는 문제가 있을 수 있습니다. 회귀보다 분류에 유용합니다.  
그러나 이상치가 있다면 평균과 표준편차에 영향을 미쳐 변환된 데이터의 확산은 매우 달라지게 된다. 따라서 이상치가 있는 경우 균형 잡힌 척도를 보장할 수 없다.
2. RobustScaler  
각 특성들의 중앙값을 0, IQR(제3사분위수-제1사분위수,Q3-Q1)을 1로 스케일링합니다. StandardScaler와 비슷하지만, 이상치의 영향을 최소화합니다.   
중앙값(median)과 IQR(interquartile range)을 사용하기 때문에 StandardScaler와 비교해보면 표준화 후 동일한 값을 더 넓게 분포 시키고 있음을 확인 할 수 있다.
3. MinMaxScaler(a,b)  
각 특성의 하한값을 a, 상한값을 b로 스케일링합니다. a=0, b=1일 경우 Normalization으로 표기할 때도 있습니다. 분류보다 회귀에 유용합니다.  다만 이상치가 있는 경우 변환된 값이 매우 좁은 범위로 압축될 수 있다.
4. MaxAbsScaler  
각 특성을 절대값이 0과 1사이가 되도록 스케일링합니다. 즉, 모든 값은 -1과 1사이로 표현되며, 데이터가 양수일 경우 MinMaxScaler와 같습니다. 큰 이상치에 민감할 수 있다.
5. Normalizer  
앞의 4가지 스케일러는 각 특성(열)의 통계치를 이용하여 진행됩니다. 그러나 Normalizer의 경우 각 샘플(행)마다 적용되는 방식입니다. 이는 한 행의 모든 특성들 사이의 유클리드 거리(L2 norm)가 1이 되도록 스케일링합니다. 일반적인 데이터 전처리의 상황에서 사용되는 것이 아니라, 모델(특히나 딥러닝) 내 학습 벡터에 적용하며, 특히나 피쳐들이 다른 단위(키, 나이, 소득 등)라면 더더욱 사용하지 않습니다. 


# 선형 회귀 모델을 위한 데이터 변환
선형 회귀 모델은 학습에 사용되는 피처와 예측값인 타겟 간에 선형 관계가 있다고 가정하는 ML 모델입니다. 따라서 선형 회귀 모델은 이 둘 간의 최적의 선형함수를 도출하는 것으로 타겟값을 예측하게 됩니다.

우리가 선형 회귀 모델을 사용하기 전에 데이터 변환이 필요한 이유는 이러한 선형 회귀 모델은 피처와 타겟값이 정규 분포 형태임을 기대하기 때문입니다.

따라서 피처 혹은 타겟값이 정규 분포를 따르지 않는 왜곡(Skew)된 형태의 분포를 가질 경우 모델의 성능이 떨어질 수 있어 모델이 데이터를 학습하기 전 데이터 변환 작업이 선행되어야 합니다.


# 데이터 질을 향상시킬만한 내용 작성
- 범주형 변수가 있는 경우 encoding 필요
- 변수간 크기가 다른 경우 scaling 필요
    * 스케일링을 하는 이유
>- 변수의 크기가 너무 작거나, 너무 큰 경우 해당 변수가 target에 미치는 영향력을 제대로 반영하기 어렵기 때문
>- 과적합된 모델은 극도로 크거나 작은 가중치를 가지는 경향 보임
- 변수가 많은 경우 feature engineering등 
- 데이터 분포를 보고(시각화 또는 왜도 첨도 확인후 ) 한쪽으로 치우쳐 있으면 아래 비대칭 데이터 처리 방법확인
> - 예측 변수와 목표 변수가 정규 분포를 따를 때 더 신뢰할 수 있는 예측이 이루어진다.
> - Box-Cox 변환
> - Square Root Transform / 루트(제곱근) 변환
> - 로그 변환
- 이상치 데이터 삭제 처리

import seaborn as sns 
import numpy as np 
import pandas as pd

df = pd.read_csv('../penguins.csv')
display(df.head())
display(df.isnull().sum())

#1.결측치 제거
missing =['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']
for i in missing:
    df[i] = df[i].fillna(df[i].median())
    
df['sex'] = df['sex'].fillna(df['sex'].mode()[0])    

# 2.범주형 데이터 변환
from sklearn.preprocessing import LabelEncoder
label = ['species','island','sex']
df[label] = df[label].apply(LabelEncoder().fit_transform)

# 3. 데이터 변환, 더미 처리
import pandas as pd
category = ['island','sex']
for i in category:
    df[i] = df[i].astype('category')    

df = pd.get_dummies(df)

# 4. 파생변수
df['body_mass_g_qcut'] = pd.qcut(df['body_mass_g'],5,labels=False)

# 5. 전처리
from sklearn.preprocessing import MinMaxScaler
scaler = ['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']
minMax = MinMaxScaler()
df[scaler] = minMax.fit_transform(df[scaler])

# 6. 데이터 분리
from sklearn.model_selection import train_test_split
feature = df.drop('species', axis = 1)
target = df['species']
X_train,X_test,y_train,y_test = train_test_split(feature,target, test_size=0.2,stratify= target)

#전처리 부분
 
1. 결측치 처리
결측치는 데이터에 따라 빈번하게 나타나는데 결측치를 처리하는 방법은 다양하다.
- 그냥 제거하는 방법
- 중위값 , 평균값으로 대체 하는 방법
- 결측치 자체를 Y 데이터로 해서 나머지로 예측하는 방법 

    경험상 예측하는 방법이 성능이 좋았던 기억이 있고 그때는 Gradient Boosting 모델을 사용했다.

2. 파생 변수 생성
데이터의 열 정보를 꼼꼼히 살펴보고 파생변수로 만들 수 있는 것이 있다면 추가해주는 것이 좋다.

예측에 있어서 열이 너무없다면 많이 늘리는 것이 중요하다.

특성교차 방법 사용

변수 두개를 곱하거나 , 원 핫 벡터를 곱한 값을 새로운 파생변수로 생성하는 방법

 * 특성 교차를 사용하면 비선형 문제를 해결 할 수 있다.
 
3. 시간 데이터 전처리
시간으로 되어있는 열은 년도, 월 , 일 , 시간 , 요일 등으로 나눠서 파생변수로 만들어주는 것이 좋다.

나눈 시간 데이터중 유의미한 데이터를 변수로 사용한다.

train['logging_timestamp'] = pd.to_datetime(train['logging_timestamp'])
train['hour'] = train['logging_timestamp'].dt.strftime('%H').astype(int)
train['day'] = train['logging_timestamp'].dt.strftime('%d').astype(int)

train = train.drop('logging_timestamp' , axis= 1)

4. 이상치 제거 ( Log 변환 )
데이터가 한쪽으로 너무 편향 되어있다면 로그를 취해주면 좋다.

로그를 적용하면 치우친 분포를 조정해주고 값들이 축소되기 때문에 이상치도 처리해줄 수 있다,

로그 변환을 했음에도 불구하고 너무 극단적인 값이 있다면 깔끔한 데이터 셋을 위해 제거해 줄 필요가 있다.

5. 불균형 데이터 처리
클래스 불균형 데이터 처리

6. 구간화
데이터의 최소값과 최대값의 차이가 크다면 구간화를 고려해볼 필요가 있다.

구간화는 pd.qcut 과 pd.cut 그리고 np.digitize 함수들이 있고 구간화를 진행하면 이상치를 완화할 수 있고

과적합을 완화 시켜주는 효과 , 결과에 대한 해석이 용이하다는 장점이 있다. 

나는 오히려 구간화를 해서 정확도가 떨어진 적이 있는 데 무조건 구간화를 하는 것은 좋지 않고 최소,최대가 차이가

많이 나거나 구간에 대한 특징이 있을 때 나눠주면 좋다.

7. 인코딩
인코딩은 고려해보는 것이 아니라 필수요소이다.

문자를 숫자로 바꿔주는 것 뿐만이아니라, label 인코딩으로 할 것인지 oneHot 인코딩으로 할 것 인지도 중요하다.

데이터에 대한 순서가 의미 있는 경우는 label로 해주는 것이 좋고, 예를 들면 학점 A학점 , B학점 A가 B보다 높다.

데이터의 순서가 의미 없는 경우는 oneHot으로 해주는 것이 좋다 , 예를 들면 강아지와 고양이

# 모델링
1. 스케일링
스케일링도 필수요소 중 하나이다. 사실 트리기반의 모델에서는 스케일링의 유무와 상관없이 결과값이 거의 차이가 없지만

다른 모델일 경우 스케일링은 중요하다. 스케일링은 일정 구간에 데이터를 다 몰아 넣음으로써 스케일을 조정해준다.

2. 변수 선택
변수가 너무 많을 경우 중요한 변수만 추려서 학습에 사용해 주는 것이 좋다.

여기서 많다는 것은 최소 100개 이상을 의미하고 정확도 측면에서 좋은 성능을 보여줄 수 도 있지만

계속해서 테스트를 진행함에 있어서 변수가 많으면 그 만큼 테스트 시간도 많이 늘어난다.

전진 선택법, 후진선택법, 단계적 선택법을 이용해서 한다

3. PCA
변수 선택의 일종으로 중요 변수만 뽑아내는 것이 아니라 차원을 축소시켜 잘 설명하는 변수들 몇개만 사용하는 방법이다.

변수 선택법과 같이 성능의 증가보다 변수들이 너무 많을 때 사용해주면 테스트의 용이하다.

4. K-fold 교차 검증
교차 검증은 말그대로 성능을 높히는 것이 아니라

내가 본 성능이 과연 다른 데이터로 섞었을 때도 비슷한 성능을 내는 지 확인하는 작업이다.

5. Hyperparams 최적화
여러가지 모델을 돌려보고 그 중 성능이 좋은 모델에 대해서 하이퍼파라미터 조합을 바꿔가며 성능을 높힐 수 있다.

Grid Search 와 Random Serach 패키지로 여러가지 조합을 테스트 해 볼 수 있고 최적의 조합을 찾아 낼 수 있다.

6. Auto ML
이건 말그대로 자동으로 제일 좋은 거 찾아주는 방법으로 다양한 라이브러리들이 있다.

데이터만 넣으면 가장 성능이 좋은 것을 찾아준다. pycarot 등이 있다.

7. Stacking
스테킹은 앙상블 기법의 하나로 예측 모델들의 결과를 트레이닝 셋으로 사용하는 것이다.

이게 무슨 말이냐 하면, 예측 모델을 쌓아서 실제로 예측한 값을 합치고 그 중 투표로 결정하는 것이다.

케글에서 순위를 다툴 때 많이 사용되지만 일반적으로는 사용을 하지 않는 편이다.		  