{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa3b6b81-265d-4554-a9b5-e4d9b47a16ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#회귀분석\n",
      "\n",
      "plt.rc(\"font\", family = \"Malgun Gothic\")\n",
      "import matplotlib\n",
      "matplotlib.rcParams['axes.unicode_minus'] = False\n",
      "\n",
      "숫자형 자료들의 기본적인 통계 자료들을 파악\n",
      "df_train.describe()\n",
      "\n",
      "문자형 자료들의 기본 정보를 파악\n",
      "# int, float 을 제외한 object만 문자형 자료다.\n",
      "df_train.describe(include='object')\n",
      "\n",
      "중복 확인\n",
      "# Checking duplicates\n",
      "print(sum(df_train.duplicated(subset = 'Id')) == 0)\n",
      "\n",
      "### 데이터 구조 Insight\n",
      "- 1460개의 행을 가지고, 81개의 열을 가지는 데이터\n",
      "- 8개의 범주형 데이터와 7개의 수치형 데이터, 1개의 이진형 데이터가 존재한다.\n",
      "- 결측치는 workclass의 경우 1836개, occupation의 경우 1843개 native.country의 경우 583개를 가짐\n",
      "- ID를 기준으로 중복된 데이터는 존재 하지 않는다.\n",
      "\n",
      "## 결측치 확인\n",
      "def check_missing_col(dataframe):\n",
      "    missing_col = []\n",
      "    for col in dataframe.columns:\n",
      "        missing_values = sum(dataframe[col].isna())\n",
      "        is_missing = True if missing_values >= 1 else False\n",
      "        if is_missing:\n",
      "            print(f'결측치가 있는 컬럼은: {col} 입니다')\n",
      "            print(f'해당 컬럼에 총 {missing_values} 개의 결측치가 존재합니다.')\n",
      "            missing_col.append([col, dataframe[col].dtype,missing_values])\n",
      "    if missing_col == []:\n",
      "        print('결측치가 존재하지 않습니다')\n",
      "    return missing_col\n",
      "\n",
      "missing_col = check_missing_col(df_train)\n",
      "pd.DataFrame(missing_col)\n",
      "\n",
      "\n",
      "# 결측치가 있는 row들을 확인합니다.\n",
      "df_train[df_train.isna().sum(axis=1) > 0]\n",
      "\n",
      "for col in missing_col:    \n",
      "    print(\"피쳐 \" , col[0])\n",
      "    print(df_train[col[0]].unique())\n",
      "    \n",
      "#결측치 시각화\n",
      "missing = df_train.isnull().sum()\n",
      "missing = missing[missing > 0]\n",
      "missing.sort_values(inplace=True)\n",
      "\n",
      "missing.plot.bar(figsize = (12,6))    \n",
      "\n",
      "plt.xlabel(\"\", fontsize = 20)\n",
      "plt.ylabel(\"\", fontsize = 20)\n",
      "plt.title(\"Total Missing Value\", fontsize = 20)\n",
      "\n",
      "for i, val in enumerate(missing.values):\n",
      "    plt.text(i,val,\"%s\"%val, horizontalalignment='center')    \n",
      "plt.show()\n",
      "\n",
      "#결측치 상관관계\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "missingdata_df  = df_train.columns[df_train.isnull().any()].tolist()\n",
      "colormap = plt.cm.PuBu\n",
      "sns.heatmap(df_train[missingdata_df].corr(),square = True, linewidths = 0.1,\n",
      "            cmap = colormap, linecolor = \"white\", vmax=0.8,annot=True, )\n",
      "plt.title(\"Correlation with Missing Values\", fontsize = 20)\n",
      "plt.show()\n",
      "\n",
      "#종속변수 시각화\n",
      "f , axes = plt.subplots(1,4)\n",
      "axes = axes.flatten()\n",
      "f.set_size_inches(20,5)\n",
      "\n",
      "# 이적료에 log\n",
      "df_train[\"log_SalePrice\"] = np.log(df_train['SalePrice'])\n",
      "\n",
      "sns.histplot(x=\"SalePrice\", data=df_train, bins=20,ax=axes[0],\n",
      "             kde=True,stat=\"density\")\n",
      "axes[0].set(title = \"SalePrice\")\n",
      "sns.histplot(x=\"log_SalePrice\", data=df_train, ax=axes[1])\n",
      "axes[1].set(title = \"log_SalePrice\")\n",
      "sns.boxplot(y=\"SalePrice\", data=df_train, ax=axes[2])\n",
      "axes[2].set(title = \"SalePrice\")\n",
      "sns.boxplot(y=\"log_SalePrice\", data=df_train, ax=axes[3])\n",
      "axes[3].set(title = \"log_SalePrice\")\n",
      "\n",
      "import scipy.stats as stats\n",
      "\n",
      "stats.shapiro(df_train['SalePrice'])\n",
      "\n",
      "# 범주형 변수별 그룹확인\n",
      "from IPython.display import display\n",
      "cate_feat = []\n",
      "num_feat = []\n",
      "for col in df_train.columns:\n",
      "    target = df_train[col]\n",
      "    if target.nunique() <=50:\n",
      "        print(col,df_train[col].dtype,target.unique())\n",
      "        display(target.value_counts().to_frame())\n",
      "#         print()\n",
      "        cate_feat.append(col)\n",
      "    else:\n",
      "        print('연속형', col, df_train[col].dtype,len(target.unique()))\n",
      "        num_feat.append(col)\n",
      "print('범주형 :', cate_feat)\n",
      "print('연속형: ', num_feat)\n",
      "\n",
      "#범주형 변수 데이터 시각화\n",
      "plt.figure(figsize=(20,70)) # 먼저 창을 만들고\n",
      "n = 1\n",
      "for col in cate_feat:\n",
      "    unique_df = df_train[col].value_counts()\n",
      "    \n",
      "    if len(unique_df) < 150:\n",
      "        ax = plt.subplot(31,2,n) # for문을 돌면서 Axes를 추가\n",
      "        unique_df.plot(kind='bar') \n",
      "        plt.title(col) \n",
      "        n+=1\n",
      "\n",
      "# plt.tight_layout()  # 창 크기에 맞게 조정        \n",
      "plt.show()         \n",
      "\n",
      "# 범주형 변수 상관관계\n",
      "from scipy.stats import chi2_contingency, chisquare\n",
      "import seaborn as sns \n",
      "\n",
      "category_feature = cate_feat.copy()\n",
      "for col in ['PoolQC' , 'MiscFeature', 'Alley', 'Fence','FireplaceQu']:\n",
      "    category_feature.remove(col)\n",
      "\n",
      "corr_list= []\n",
      "\n",
      "for i in category_feature:\n",
      "    c_list = []\n",
      "    for j in category_feature:\n",
      "        ct = pd.crosstab(df_train[i],df_train[j])\n",
      "        X2=chi2_contingency(observed=ct)[0]\n",
      "        n = len(df_train)\n",
      "        minDim = min(len(df_train[i].unique()),len(df_train[j].unique()))-1\n",
      "        c = np.sqrt((X2/n) / minDim)\n",
      "#         c = np.sqrt(result[0]/(len(train)*(min(len(train[i].unique()),len(train[j].unique()))-1)))\n",
      "        c_list.append(c)\n",
      "    corr_list.append(c_list)\n",
      "    \n",
      "corr_df = pd.DataFrame(corr_list,columns=category_feature, index=category_feature)\n",
      "\n",
      "sns.set(rc = {'figure.figsize':(20,12)})\n",
      "sns.heatmap(corr_df,vmin=-1,vmax=1,cmap='RdBu',linewidths=.1,annot=True, fmt='.2f')\n",
      "\n",
      "#연속형 변수 시각화\n",
      "import warnings\n",
      "warnings.filterwarnings(action='ignore')\n",
      "n = 1\n",
      "\n",
      "plt.figure(figsize=(20,70)) # 먼저 창을 만들고\n",
      "for col in num_feat:\n",
      "    ax = plt.subplot(22,2,n)\n",
      "    sns.distplot(df_train.loc[df_train[col].notnull(), col])\n",
      "    plt.title(col)\n",
      "    n += 1\n",
      "    \n",
      "plt.tight_layout()  # 창 크기에 맞게 조정         \n",
      "plt.show()\n",
      "\n",
      "# 연속형 자료 boxplot\n",
      "import warnings\n",
      "warnings.filterwarnings(action='ignore')\n",
      "n = 1\n",
      "\n",
      "plt.figure(figsize=(20,70)) # 먼저 창을 만들고\n",
      "for col in num_feat:\n",
      "    ax = plt.subplot(22,2,n)    \n",
      "    sns.boxplot(data = df_train, y=col, ax=ax)\n",
      "    plt.title(col)\n",
      "    n += 1\n",
      "    \n",
      "plt.tight_layout()  # 창 크기에 맞게 조정         \n",
      "plt.show()\n",
      "\n",
      "#범주형 변수 및 종속변수 boxplot\n",
      "li_cat_feats = list(cate_feat)\n",
      "nr_rows = 15\n",
      "nr_cols = 3\n",
      "\n",
      "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n",
      "\n",
      "for r in range(0,nr_rows):\n",
      "    for c in range(0,nr_cols):  \n",
      "        i = r*nr_cols+c\n",
      "        if i < len(li_cat_feats):\n",
      "            sns.boxplot(x=li_cat_feats[i], y=df_train[\"SalePrice\"], \n",
      "                        data=df_train, ax = axs[r][c])\n",
      "    \n",
      "plt.tight_layout()    \n",
      "plt.show()\n",
      "\n",
      "#연속형 변수 상관관계\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "corr_df = df_train.corr()\n",
      "\n",
      "# 사이즈 조정\n",
      "sns.set(rc={'figure.figsize':(25,12)})\n",
      "\n",
      "# 절반만 표시하기 위한 mask 설정\n",
      "mask=np.zeros_like(corr_df, dtype=np.bool)\n",
      "mask[np.triu_indices_from(mask)]=True\n",
      "\n",
      "# ax = sns.heatmap(corr_df,\n",
      "#                  annot=True, # 데이터 값 표시\n",
      "#                  mask=mask, # 마스크 적용 표시\n",
      "#                  cmap=plt.cm.PuBu)\n",
      "\n",
      "sns.heatmap(df_train.corr(),square = True, linewidths = 0.1,\n",
      "            cmap = colormap, linecolor = \"white\", vmax=0.8,annot=True)\n",
      "\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Relationship of cols', fontsize=20)\n",
      "plt.show() \n",
      "\n",
      "#변수가 많을 경우 종속변수와 상관계수가 높은 순으로 히트맵 그려보자\n",
      "k= 11\n",
      "cols = df_train.corr().nlargest(k,'SalePrice')['SalePrice'].index\n",
      "print(cols)\n",
      "cm = np.corrcoef(df_train[cols].values.T)\n",
      "f , ax = plt.subplots(figsize = (12,10))\n",
      "sns.heatmap(cm, vmax=.8, linewidths=0.1,square=True,annot=True,cmap=colormap,\n",
      "            linecolor=\"white\",xticklabels = cols.values ,\n",
      "            annot_kws = {'size':14},yticklabels = cols.values)\n",
      "\n",
      "\n",
      "# null 처리\n",
      "# Null 이 너무 많은 컬럼들과 불필요한 컬럼 삭제\n",
      "df_train = df_train.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', \n",
      "                          'Fence','FireplaceQu'], axis=1 )\n",
      "df_test = df_test.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', \n",
      "                        'Fence','FireplaceQu'], axis=1 )\n",
      "\n",
      "# Drop 하지 않는 숫자형 Null컬럼들은 평균값으로 대체\n",
      "df_train.fillna(df_train.mean(),inplace=True)\n",
      "df_test.fillna(df_test.mean(),inplace=True)\n",
      "\n",
      "# Null 값이 있는 피처명과 타입을 추출\n",
      "null_column_count = df_train.isnull().sum()[df_train.isnull().sum() > 0]\n",
      "print('## Null 피처의 Type :\\n', df_train.dtypes[null_column_count.index])\n",
      "'''\n",
      "문자형 피처를 제외 하고는 null 값이 없다. 문자형 피처는 원핫 인코딩으로 변환..\n",
      "원한 인코딩으로 변화를 하면 null 값은 자동으로 'None' 칼럼으로 대체 해주기 때문에 \n",
      "별도의 null 처리가 필요없다. \n",
      "'''\n",
      "\n",
      "\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error,\\\n",
      "                mean_absolute_percentage_error, r2_score\n",
      "\n",
      "# log 값 변환 시 NaN등의 이슈로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산\n",
      "def rmsle(y, pred):\n",
      "    log_y = np.log1p(y)\n",
      "    log_pred = np.log1p(pred)\n",
      "    squared_error = (log_y - log_pred) ** 2\n",
      "    rmsle = np.sqrt(np.mean(squared_error))\n",
      "    return rmsle\n",
      "\n",
      "# 사이킷런의 mean_square_error() 를 이용하여 RMSE 계산\n",
      "def rmse(y,pred):\n",
      "    return np.sqrt(mean_squared_error(y,pred))\n",
      "\n",
      "# MSE, RMSE, RMSLE 를 모두 계산 \n",
      "def evaluate_regr(y,pred):\n",
      "    rmsle_val = rmsle(y,pred)\n",
      "    rmse_val = rmse(y,pred)\n",
      "    # MAE 는 scikit learn의 mean_absolute_error() 로 계산\n",
      "    mae_val = mean_absolute_error(y,pred)\n",
      "    mse = mean_squared_error(y,pred)\n",
      "    mape = mean_absolute_percentage_error(y,pred)\n",
      "    r2 = r2_score(y,pred)\n",
      "    print('RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.\n",
      "                              format(rmsle_val, rmse_val, mae_val))\n",
      "    print('MSE :{0:.3F}, MAPE :{1:.3F}, R2 :{2:.3F}  '.\n",
      "                              format(mse, mape, r2))\n",
      "    \n",
      "# 모델과 학습/테스트 데이터 셋을 입력하면 성능 평가 수치를 반환\n",
      "def get_model_predict(model, X_train, X_test, y_train, \n",
      "                                  y_test, is_expm1=False):\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_test)\n",
      "    if is_expm1 :\n",
      "        y_test = np.expm1(y_test)\n",
      "        pred = np.expm1(pred)\n",
      "    print('###',model.__class__.__name__,'###')\n",
      "    evaluate_regr(y_test, pred)\n",
      "# end of function get_model_predict      \n",
      "\n",
      "def get_top_error_data(y_test, pred, n_tops = 5):\n",
      "    # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. \n",
      "    result_df = pd.DataFrame(y_test.values, columns=['real_count'])\n",
      "    result_df['predicted_count']= np.round(pred)\n",
      "    result_df['diff'] = np.abs(result_df['real_count'] - \n",
      "                                   result_df['predicted_count'])\n",
      "    # 예측값과 실제값이 가장 큰 데이터 순으로 출력. \n",
      "    print(result_df.sort_values('diff', ascending=False)[:n_tops])\n",
      "    \n",
      "def get_top_bottom_coef(model, n=10):\n",
      "    # coef_ 속성을 기반으로 Series 객체를 생성. index는 컬럼명. \n",
      "    coef = pd.Series(model.coef_, index=X_features.columns)\n",
      "    \n",
      "    # + 상위 10개 , - 하위 10개 coefficient 추출하여 반환.\n",
      "    coef_high = coef.sort_values(ascending=False).head(n)\n",
      "    coef_low = coef.sort_values(ascending=False).tail(n)\n",
      "    return coef_high, coef_low\n",
      "\n",
      "def visualize_coefficient(models):\n",
      "    \n",
      "    # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 회귀 계수 시각화. \n",
      "    for i_num, model in enumerate(models):\n",
      "         # 3개 회귀 모델의 시각화를 위해 3개의 컬럼을 가지는 subplot 생성\n",
      "        fig, axs = plt.subplots(figsize=(10,8),nrows=1, ncols=1)        \n",
      "        # 상위 10개, 하위 10개 회귀 계수를 구하고, 이를 판다스 concat으로 결합. \n",
      "        coef_high, coef_low = get_top_bottom_coef(model)\n",
      "#         print(coef_high, coef_low)\n",
      "        coef_concat = pd.concat( [coef_high , coef_low] )\n",
      "        # 순차적으로 ax subplot에 barchar로 표현. 한 화면에 표현하기 위해 \n",
      "        # tick label 위치와 font 크기 조정. \n",
      "        axs.set_title(model.__class__.__name__+' Coeffiecents', size=25)\n",
      "        \n",
      "        for label in (axs.get_xticklabels() + axs.get_yticklabels()):\n",
      "            label.set_fontsize(22)\n",
      "            \n",
      "        sns.barplot(x=coef_concat.values, y=coef_concat.index , ax=axs)\n",
      "        \n",
      "        \n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "y_target = df_train_ohe_df['SalePrice']\n",
      "X_features = df_train_ohe_df.drop(['SalePrice'], axis=1)\n",
      "\n",
      "X_train, X_test, y_train,y_test = train_test_split(X_features,\n",
      "                                y_target,test_size=0.3, random_state=123)\n",
      "\n",
      "lr_reg = LinearRegression()\n",
      "lr_reg.fit(X_train,y_train)\n",
      "pred = lr_reg.predict(X_test)\n",
      "\n",
      "evaluate_regr(y_test,pred)\n",
      "\n",
      "# 실제값과 예측값이 어느정도 차이가 나는지 확인\n",
      "get_top_error_data(y_test,pred)\n",
      "\n",
      "# 종속변수 로그 변환후 학습\n",
      "y_target_log = np.log1p(y_target)\n",
      "\n",
      "sns.histplot(y_target_log, bins=20,kde=True,stat=\"density\")\n",
      "plt.show()\n",
      "\n",
      "# 전체 데이터 셋으로 교차 검증수행\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "def get_avg_rmse_cv(models):\n",
      "    for model in models:\n",
      "        # 분할하지 않고 전체 데이터로 cross_val_score( ) 수행. 모델별 CV RMSE값과 평균 RMSE 출력\n",
      "        rmse_list = np.sqrt(-cross_val_score(model, X_features, y_target_log,\n",
      "                                scoring=\"neg_mean_squared_error\", cv = 5))\n",
      "        rmse_avg = np.mean(rmse_list)\n",
      "        print('\\n{0} CV RMSE 값 리스트: {1}'.format( \n",
      "                                model.__class__.__name__, np.round(rmse_list, 3)))\n",
      "        print('{0} CV 평균 RMSE 값: {1}'.format( \n",
      "                                model.__class__.__name__, np.round(rmse_avg, 3)))\n",
      "\n",
      "# 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 CV RMSE값 출력           \n",
      "models = [lr_reg, ridge_reg, lasso_reg]\n",
      "get_avg_rmse_cv(models)\n",
      "\n",
      "\n",
      "# 하이퍼 파라미터 튜닝\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "def print_best_params(model, params):\n",
      "    grid_model = GridSearchCV(model, param_grid=params, \n",
      "                              scoring='neg_mean_squared_error', cv=5)\n",
      "    grid_model.fit(X_features, y_target_log)\n",
      "    rmse = np.sqrt(-1* grid_model.best_score_)\n",
      "    print('{0} 5 CV 시 최적 평균 RMSE 값: {1}, 최적 alpha:{2}'.format(\n",
      "                                        model.__class__.__name__,\n",
      "                                np.round(rmse, 4), grid_model.best_params_))\n",
      "    return grid_model.best_estimator_\n",
      "\n",
      "ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }\n",
      "lasso_params = { 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03,\n",
      "                                  0.1, 0.5, 1,5, 10] }\n",
      "\n",
      "best_rige = print_best_params(ridge_reg, ridge_params)\n",
      "best_lasso = print_best_params(lasso_reg, lasso_params)\n",
      "\n",
      "#모델 수행\n",
      "from sklearn.linear_model import Ridge,Lasso\n",
      "\n",
      "# LinearRegression, Ridge, Lasso 학습, 예측, 평가\n",
      "\n",
      "lr_reg = LinearRegression()\n",
      "lr_reg.fit(X_train, y_train)\n",
      "\n",
      "ridge_reg = Ridge(alpha=12)\n",
      "ridge_reg.fit(X_train, y_train)\n",
      "\n",
      "lasso_reg = Lasso(alpha=0.001)\n",
      "lasso_reg.fit(X_train, y_train)\n",
      "\n",
      "models = [lr_reg, ridge_reg, lasso_reg]\n",
      "\n",
      "for i_num, model in enumerate(models):\n",
      "    get_model_predict(model,X_train, X_test, y_train, y_test,True)        \n",
      "    \n",
      "    \n",
      "# Feature 분포도 및 이상치 데이터 처리 후\n",
      "from scipy.stats import skew\n",
      "\n",
      "numerical_feature = list(set(df_train.columns) - set(category_feature) - \n",
      "                                     set(['Id', 'SalePrice']))\n",
      "\n",
      "# data[numerical_feature].skew()\n",
      "skew_features = df_train[numerical_feature].apply(lambda x : skew(x))\n",
      "'''\n",
      "skew(왜곡)정도가 1이상인 칼럼만 추출\n",
      "'''\n",
      "skew_features_top = skew_features[skew_features > 1]\n",
      "skew_features_top.sort_values(ascending=False)\n",
      "\n",
      "from scipy.stats import skew\n",
      "\n",
      "# data[numerical_feature].skew()\n",
      "skew_features_test = df_test[numerical_feature].apply(lambda x : skew(x))\n",
      "'''\n",
      "skew(왜곡)정도가 1이상인 칼럼만 추출\n",
      "'''\n",
      "skew_features_test_top = skew_features_test[skew_features_test > 1]\n",
      "skew_features_test_top.sort_values(ascending=False)\n",
      "\n",
      "df_train[skew_features_top.index] = np.log1p(df_train[skew_features_top.index])\n",
      "df_test[skew_features_test_top.index] = \n",
      "                                np.log1p(df_test[skew_features_test_top.index])\n",
      "    \n",
      "X_features = df_train_ohe_df.drop(['SalePrice'], axis=1)\n",
      "\n",
      "X_train, X_test, y_train,y_test = train_test_split(X_features,\n",
      "                                y_target_log,test_size=0.3, random_state=123)\n",
      "\n",
      "ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }\n",
      "lasso_params = { 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03,\n",
      "                          0.1, 0.5, 1,5, 10] }\n",
      "\n",
      "best_rige = print_best_params(ridge_reg, ridge_params)\n",
      "best_lasso = print_best_params(lasso_reg, lasso_params)    \n",
      "\n",
      "\n",
      "#\n",
      "\n",
      "from xgboost import XGBRegressor\n",
      "xgb_param ={'n_estimators':[1000]}\n",
      "\n",
      "xgb_reg = XGBRegressor(n_estimators =1000,learning_rate =0.05, \n",
      "                       colsample_bytree=0.5, subsample=0.8)\n",
      "best_xgb = print_best_params(xgb_reg,xgb_param)\n",
      "\n",
      "from lightgbm import LGBMRegressor\n",
      "lgb_param ={'n_estimators':[1000]}\n",
      "lgb_reg = LGBMRegressor(n_estimators =1000,learning_rate =0.05,\n",
      "                        num_leaves=4, colsample_bytree=0.4, \n",
      "                        subsample=0.6, reg_lambda=10, n_jobs=-1)\n",
      "best_lgbm = print_best_params(lgb_reg,lgb_param)\n",
      "\n",
      "# 모델의 중요도 상위 20개의 피처명과 그때의 중요도값을 Series로 반환.\n",
      "def get_top_features(model):\n",
      "    ftr_importances_values = model.feature_importances_\n",
      "    ftr_importances = pd.Series(ftr_importances_values, index=X_features.columns  )\n",
      "    ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n",
      "    return ftr_top20\n",
      "\n",
      "def visualize_ftr_importances(models):\n",
      "    # 2개 회귀 모델의 시각화를 위해 2개의 컬럼을 가지는 subplot 생성\n",
      "    fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=2)\n",
      "    fig.tight_layout() \n",
      "    # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 피처 중요도 시각화. \n",
      "    for i_num, model in enumerate(models):\n",
      "        # 중요도 상위 20개의 피처명과 그때의 중요도값 추출 \n",
      "        ftr_top20 = get_top_features(model)\n",
      "        axs[i_num].set_title(model.__class__.__name__+' Feature Importances', size=25)\n",
      "        #font 크기 조정.\n",
      "        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):\n",
      "            label.set_fontsize(22)\n",
      "        sns.barplot(x=ftr_top20.values, y=ftr_top20.index , ax=axs[i_num])\n",
      "\n",
      "# 앞 예제에서 print_best_params( )가 반환한 GridSearchCV로 최적화된 모델의 피처 중요도 시각화    \n",
      "models = [best_xgb, best_lgbm]\n",
      "visualize_ftr_importances(models)\n",
      "\n",
      "\n",
      "# 회귀 모델의 예측 결과 혼합을 통한 최종 예측\n",
      "# MSE, RMSE, RMSLE 를 모두 계산 \n",
      "def evaluate_regr(y,pred, is_expm1=False):\n",
      "    if is_expm1 :\n",
      "        y = np.expm1(y)\n",
      "        pred = np.expm1(pred)\n",
      "        \n",
      "    rmsle_val = rmsle(y,pred)\n",
      "    rmse_val = rmse(y,pred)\n",
      "    # MAE 는 scikit learn의 mean_absolute_error() 로 계산\n",
      "    mae_val = mean_absolute_error(y,pred)\n",
      "    mse = mean_squared_error(y,pred)\n",
      "    mape = mean_absolute_percentage_error(y,pred)\n",
      "    r2 = r2_score(y,pred)\n",
      "    print('RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.\n",
      "                                    format(rmsle_val, rmse_val, mae_val))\n",
      "    print('MSE :{0:.3F}, MAPE :{1:.3F}, R2 :{2:.3F}  '.\n",
      "                                      format(mse, mape, r2))   \n",
      "\n",
      "# 개별 모델의 학습\n",
      "ridge_reg = Ridge(alpha=8)\n",
      "ridge_reg.fit(X_train, y_train)\n",
      "lasso_reg = Lasso(alpha=0.001)\n",
      "lasso_reg.fit(X_train, y_train)\n",
      "# 개별 모델 예측\n",
      "ridge_pred = ridge_reg.predict(X_test)\n",
      "lasso_pred = lasso_reg.predict(X_test)\n",
      "\n",
      "# 개별 모델 예측값 혼합으로 최종 예측값 도출\n",
      "pred = 0.5 * ridge_pred + 0.5 * lasso_pred\n",
      "preds = {'최종 혼합': pred,\n",
      "         'Ridge': ridge_pred,\n",
      "         'Lasso': lasso_pred}\n",
      "#최종 혼합 모델, 개별모델의 RMSE 값 출력\n",
      "evaluate_regr(y_test, pred,True)    \n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import ssl\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "textPage = urlopen(\n",
    "        \"https://raw.githubusercontent.com/parkheungki/StudyRep/master/regression.txt\", context=context)\n",
    "\n",
    "# 인코딩 방식 추출(명시되어 있지 않으면 utf-8을 사용한다.)\n",
    "encoding = textPage.info().get_content_charset(failobj='utf-8')\n",
    "\n",
    "string = textPage.read()\n",
    "string2 = string.decode(encoding)\n",
    "\n",
    "# print(type(string), string)\n",
    "print(string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2964ca71-b649-4cd5-8be0-0dc824e24c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidenceLevel = .95\n",
      "numOfTails      = 2\n",
      "alpha           = (1 - confidenceLevel)/numOfTails\n",
      "z_critical      = stats.norm.ppf(1 - alpha)\n",
      "\n",
      "mean = 199.5\n",
      "sigma = 5 / np.sqrt(50)\n",
      "\n",
      "# confidence interval formula (manual)\n",
      "lowerCI = mean - (z_critical * sigma)\n",
      "upperCI = mean + (z_critical * sigma)\n",
      "\n",
      "#  print confidence intervals\n",
      "print('Confidence Level:\t{:.0%}'.format(confidenceLevel))\n",
      "print('Number of Tails:\t{}'.format(numOfTails))\n",
      "print('alpha:\t\t\t{:.4f}'.format(alpha))\n",
      "print('z-critical value:\t{:.4f}  <---'.format(z_critical))\n",
      "print('\n",
      "Confidence Interval:\n",
      "lower CI\t\t{:.4f}'.format(lowerCI))\n",
      "print('upper CI:\t\t{:.4f}'.format(upperCI))\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import ssl\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "textPage = urlopen(\n",
    "        \"https://raw.githubusercontent.com/parkheungki/test/main/test.txt\", context=context)\n",
    "\n",
    "string = textPage.read()\n",
    "string2 = string.decode('unicode_escape')\n",
    "\n",
    "# print(type(string), string)\n",
    "print(string2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
